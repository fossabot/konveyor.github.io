[{"uri":"http://konveyor.github.io/crane/","title":"Crane","tags":[],"description":"","content":"Crane Use this section to better understand and use the Konveyor Crane tool.\n"},{"uri":"http://konveyor.github.io/forklift/","title":"Forklift","tags":[],"description":"","content":"Forklift Use this section to better understand and use the Konveyor Forklift tool.\n"},{"uri":"http://konveyor.github.io/move2kube/","title":"Move2Kube","tags":[],"description":"","content":"Move2Kube Use this section to better understand and use the Konveyor Move2Kube tool.\nFor more information, documentation, and tutorials, visit the Move2Kube documentation site\n"},{"uri":"http://konveyor.github.io/tackle/tackle1/","title":"Tackle 1.0","tags":[],"description":"","content":"Tackle 1.0 Use this section to better understand and use the Konveyor Tackle tool version 1.0.\n"},{"uri":"http://konveyor.github.io/tackle/","title":"Tackle 1.0 &amp; 2.0","tags":[],"description":"","content":"Tackle Use this section to better understand and use the Konveyor Tackle tool.\n"},{"uri":"http://konveyor.github.io/tackle/tackle2/","title":"Tackle 2.0","tags":[],"description":"","content":"Tackle 2.0 Use this section to better understand and use the Konveyor Tackle tool version 2.0.\n"},{"uri":"http://konveyor.github.io/pelorus/","title":"Pelorus","tags":[],"description":"","content":"Pelorus Use this section to better understand and use the Konveyor Pelorus tool.\nComing soon\u0026hellip;\n"},{"uri":"http://konveyor.github.io/move2kube/installation/","title":"Installation","tags":[],"description":"","content":"For installation instructions visit https://move2kube.konveyor.io/installation.\n"},{"uri":"http://konveyor.github.io/move2kube/overview/","title":"Overview","tags":[],"description":"","content":"The Move2Kube tool helps application owners migrate legacy workloads to run on Kubernetes clusters and eventually automate their deployments after multiple iterations. It analyzes Docker Compose files, Cloud Foundry manifest files, and even source code to generate Kubernetes deployment files including object YAML files, Helm charts, and operators.\nMove2Kube has a very modular architecture making it easy to custom functionality for a large variety of migration use-cases.\nThe project includes three tools:\nmove2kube: The primary tool is the command line interface (CLI) that takes in application source code and generates Kubernetes artifacts. move2kube-ui: A UI for interacting with the Move2Kube CLI tool for running fully-managed Move2Kube runtimes. move2kube-transformers: A collection of useful transformers for extending Move2Kube\u0026rsquo;s functionality that has been built by the Konveyor community based on experience from performing migrations for clients. For more information, documentation, and tutorials, visit https://move2kube.konveyor.io/\nSource\n"},{"uri":"http://konveyor.github.io/tackle/overview/","title":"Overview","tags":[],"description":"","content":"Tackle is a collection of tools that support modernizing and migrating applications to Kubernetes. These tools assess applications to determine which option is the appropriate migration strategy for each application:\nRehosting Replatforming Refactoring Tackle uses an interactive questionnaire for the assessment which enables key stakeholders to gather information about applications, discuss risks flagged by Tackle, and reach a consensus in formulating recommendations for each application.\nTackle Refactoring Tools The tools are cloud-native micro-services that are accessible from a common Tackle UI.\nApplication Inventory Pathfinder Controls Application Inventory Tackle Application Inventory is the vehicle which selects applications for assessment by Pathfinder. It provides users four main functions:\nMaintain a portfolio of applications. Link applications to the business services they support. Define interdependencies. Add metadata using an extensible tagging model to describe and categorize applications in multiple dimensions. Pathfinder Tackle Pathfinder is an interactive questionnaire based tool that assesses the suitability of applications for modernization in order to deploy them into containers on an enterprise Kubernetes platform. The tool output includes::\nApplication’s suitability for Kubernetes Associated risks Adoption plan with the applications’ prioritization, business criticality and dependencies. Controls Tackle Controls are a collection of entities that add value to Application Inventory and the Pathfinder assessment. They comprise business services, stakeholders, stakeholder groups, job functions, tag types, and tags.\nTackle Controls are a collection of entities that add value to the Application Inventory and the Pathfinder assessment including:\nBusiness services Stakeholders Stakeholder groups Job functions Tag types Tag Tackle Projects Tackle Web UI Tackle Application Inventory Tackle Pathfinder Tackle Controls Tackle Documentation Tackle Commons REST Tackle Keycloak Theme Tackle DiVA Tackle Test Generator Tackle Container Advisor Source\n"},{"uri":"http://konveyor.github.io/crane/overview/","title":"Overview","tags":[],"description":"","content":"The Crane tool helps application owners migrate Kubernetes workloads and their states between clusters, remove environment-specific configuration, and automate application deployments along the way.\nThe process uses a few tools:\ncrane: The command line tool that migrates applications to the terminal. crane-lib: The brains behind Crane functionality responsible for transforming resources. crane-plugin-openshift: Plugin specifically tailored to manage OpenShift migration workloads and an example of a repeatable best-practice. crane-plugins: Collection of plugins from the Konveyor community based on experience from performing Kube migrations. Why crane is needed? Crane is the product of several years of experience performing large-scale production Kubernetes migrations that are large, complex, error-prone, and usually peformed in a limited window of time.\nTo face those challenges, the Crane migration tool is designed with transparency and ease-of-diagnostics in mind. It drives migration through a pipeline of non-destructive tasks that output results to disk so the operation can be easily audited and versioned without ever impacting live workloads. The tasks can be run repeatedly and will output consistent results given the same inputs without side-effects on the system at large.\nSource\n"},{"uri":"http://konveyor.github.io/forklift/overview/","title":"Overview","tags":[],"description":"","content":"Forklift provides a simplistic way of large scale migration virtual machines at scale to Kubernetes from traditional hypervisors like VMware (vSphere) and Red Hat Virtualization (oVirt) to KubeVirt. Forklift is an open source tool based on proven core linux v2v technologies that can rehost a few or hundreds of VMs while minimizing downtime as it lifts and shifts the applications. The benefits of Forklift include:\nMigration Analytics: Validate the compatibility of the virtual machine before executing the migration. Warm migrations: Reduce downtime by using warm migration capabilities, which will pre-copy the data before finalizing the migration. Migration at Scale: Migrate small or large amount of virtual machines at once. Forklift operates using the functionality of multiple projects:\nForklift Operator. The Forklift Operator deploys and maintains Forklift. Forklift UI. The Forklift UI is based on Patternfly 4. Forklift Controller. The Forklift Controller orchestrates the migration. Forklift Validation. The Forklift Validation service checks the virtual machines for possible issues before migration. Runs with Open Policy Agent. Forklift must-gather. Support tool for gathering information about the Forklift environment. Additional resources\nPerformance recommendations for migrating from VMware vSphere to OpenShift Virtualization. Performance recommendations for migrating from Red Hat Virtualization to OpenShift Virtualization. Migration Types Forklift supports cold migration from oVirt (oVirt) and warm migration from both VMware vSphere and from oVirt.\nCold migration Cold migration is the default migration type where the source\u0026rsquo;s virtual machines are shut down while the data is copied.\nWarm migration Warm migration copies most of the data during the precopy stage while the source virtual machines (VMs) are running. During a time of minimal data transactions, the cutover stage shuts down the VMs and the remaining data is copied.\nMigration Stages Forklift operates in two stages, precopy and cutover.\nPrecopy stage The VMs are not shut down during the precopy stage. Instead the VM disks are copied incrementally using changed block tracking (CBT) snapshots. The snapshots are created at one-hour intervals by default but can be changed by updating the forklift-controller deployment.\nImportant CBT must be enabled for each source VM and each VM disk.\nA VM can support up to 28 CBT snapshots. If the source VM has too many CBT snapshots and the Migration Controller service is not able to create a new snapshot, warm migration might fail. The Migration Controller service deletes each snapshot when it is no longer required.\nThe precopy stage runs until the cutover stage is started manually or is scheduled to start.\nCutover stage The VMs are shut down during the cutover stage and the remaining data is migrated. Data stored in RAM is not migrated.\nStart the cutover stage manually by using the Forklift console or by scheduling a cutover time in the Migration manifest.\nSource\n"},{"uri":"http://konveyor.github.io/tackle/upgradeto2/","title":"Upgrading from 1.2 to 2.0","tags":[],"description":"","content":" For additional information, see the Tackle CLI tool directory in the tackle2-hub repository.\nA CLI tool is available to upgrade Tackle that will migrate application data seeds from a running 1.2 instance to a running 2.0 instance, or export the data to be imported at a later time. The tool has the following functions:\nexport-tackle1: Exports Tackle 1.2 API objects into local JSON files (tags, tag types, and job functions). tackle --skip-destination-check export-tackle1: Exports Tackle 1.2 API objects into a local directory. import: Creates objects in Tackle 2 from local JSON files. clean: Deletes objects uploaded to Tackle 2 from local JSON files. clean-all: Deletes all data returned by Tackle 2 including seeds and additional data to clean, and skips some Pathfinder items without index action. Export/Import To export data from a running 1.2 Tackle instance and import it into a running 2.0 instance, the following commands are used.\n* tackle export-tackle1 * tackle import The export-tackle1 command identifies seed resources in the Tackle2 instance first, then downloads all resources from Tackle 1.2 API, transforms it to the format expected by the Tackle 2 Hub, and lastly re-maps resources to the seeds already existing in the destination Tackle2 Hub API. The result is serialized and stored into local JSON files.\nThe tackle import command connects to Tackle2 Hub, checks existing objects for possible collisions (by IDs) and uploads resources from local JSON files.\nDelayed Export If a Tackle 2.0 instance is not available or one needs to be cleaned up, all 1.2 instance data is exported to a local directory regardless of what already exists in Tackle 2.0. To export data for delayed import, the following commands are used.\n* tackle --skip-destination-check export-tackle1 * tackle clean-all * tackle import See the CLI options section below for more tool functions.\nGit Python 3 with a YAML parser RHEL8/Python 3.6: Parser included RHEL9/Python 3.9: PyYAML module needs to be installed Verifying and installing YAML Parser Follow the steps below to install a YAML parser.\nNote: Use the python3 -m pip install pyyaml command to verify a parser is installed or to install the parser on non-RHEL-like distributions that have the dnf command.\nInstall parser for RHEL-like Linux systems. dnf install python39 python3-pyyaml git Using the Upgrade tool Follow these procedures to download and start the upgrade tool, then export the 1.2 Tackle instance objects and import them into the Tackle 2.0 instance.\nGetting started Follow the steps below to clone the tool repository, set credentials, and run the tool.\nUse the tackle-config.yml.example file as a template to set your Tackle endpoints and credentials and save it as tackle-config.yml.\nProcedure\nClone Github repository. git clone https://github.com/konveyor/tackle2-hub.git Change to the tool directory. cd hack/tool Set API endpoints and credentials in a config file (tackle-config.yml by default). # Main Tackle 2 endpoint and credentials url: https://tackle-konveyor-tackle.apps.cluster.local username: admin password: # Tackle 1.2 endpoint and credentials, e.g. to dump data and migrate to Tackle2 tackle1: url: https://tackle-tackle.apps.mta01.cluster.local username: tackle password: Run the tackle tool. ./tackle Export Tackle 1.2 Run tackle export-tackle1 to create a data dump of Tackle 1.2 objects into JSON files in a default local directory: ./tackle-data.\nNote: Unverified HTTPS warnings from Python could be hidden by export PYTHONWARNINGS=\u0026ldquo;ignore:Unverified HTTPS request\u0026rdquo; or with the -w command option.\nProcedure\nRun the tackle export on the 1.2 instance. tackle export-tackle1 Review the JSON files in the ./tackle-data directory and modify if necessary. Import to Tackle 2 Hub After checking the local JSON dump files in ./tackle-data directory (if needed), use the command below to create objects in Tackle 2 Hub by running tackle import.\nProcedure\nRun the tackle export on the 1.2 instance. tackle import Import troubleshooting The tackle import command could fail in a pre-import check phase which ensures there are no resources of a given type with the same ID in the Tackle 2.0 destination (error after checking tagtypes in destination Tackle2.. etc.). In this case, run the tackle clean command, to remove these objects from the destination API, or remove it manually either from the destination or from the JSON data files.\nIf the import failed in the upload phase (error after Uploading tagtypes.. etc.), try running tackle clean to remove already imported objects followed by tackle clean-all which lists all resources of all known data types in the destination Tackle 2 API and deletes it without looking to local data files.\nCleaning objects Warning: All clean actions may delete objects already present in the Tackle 2.0 that are unrelated to the import data.\nClean objects In the event existing objects in the Tackle 2.0 instance are in conflict with the Tackle 1.2 JSON dump files, the following command will delete objects previously created by the import command.\nProcedure\nRun the tackle export on the 1.2 instance. tackle clean Clean all objects Follow the steps below to list objects from all data types and delete those resources to clean the Tackle 2.0 instance.\nNote: The clean-all command deletes all resources from the Tackle 2.0 Hub API, however, the Pathfinder API does not support listing assessments without providing an applicationID. The applications may not be present in Hub, so an \u0026ldquo;orphaned\u0026rdquo; assessments could stay in Pathfinder. In order to resolve potential collision with imported data, run tackle clean together with the clean-all command.\nProcedure\nRun the tackle export on the 1.2 instance. tackle clean-all Review the JSON files in the ./tackle-data directory. Command line options The upgrade tool has the following command line options.\n-c / --config: This path specifies a YAML file with configuration options including endpoints and credentials for Tackle APIs (tackle-config.yml by default).\n-v / --verbose: Verbose output option logs all API requests and responses providing more information for possible debugging (off by default).\n-d / --data-dir: Data directory specifies a path to a local directory for Tackle database records in JSON format (./tackle-data by default).\n-s / --skip-destination-check: Creates a full export without having access to the Tackle 2.0 destination and executes all seed objects with this option. When importing this data, the destination needs to be empty (run clean-all first).\n-w / --disable-ssl-warnings: Supress SSL warnings option for api requests.\n-i / --ignore-import-errors: Import errors can be skipped.\nWarning: This option is not recommended. Only use with high attention to avoid data inconsistency. If the import has failed, it is recommended use the tackle clean command to only remove imported resources.\nExample:\n$ tackle --help usage: tackle [-h] [-c [CONFIG]] [-d [DATA_DIR]] [-v] [-s] [action ...] Konveyor Tackle maintenance tool. positional arguments: action One or more Tackle commands that should be executed, options: export-tackle1 import clean clean-all options: -h, --help show this help message and exit -c [CONFIG], --config [CONFIG] A config file path (tackle-config.yml by default). -d [DATA_DIR], --data-dir [DATA_DIR] Local Tackle data directory path (tackle-data by default). -v, --verbose Print verbose output (including all API requests). -s, --skip-destination-check Skip connection and data check of Tackle 2 destination. -w, --disable-ssl-warnings Do not display warnings during ssl check for api requests. -i, --ignore-import-errors Skip to next item if an item fails load. "},{"uri":"http://konveyor.github.io/tackle/tackle2/admintasks/","title":"Administrator view tasks","tags":[],"description":"","content":"The administrator view is intended to be used by administrators to set up the Tackle instance environment. Credentials This management module enables administrators to create and manage credentials for access to private repositories. It also allows for the architects to assign the credentials to applications without knowing their contents. Configuring source control credentials Follow the steps below to create new credentials for a source control repository.\nProcedure\nClick Credentials in the left menu of the Administrator view. Click the Create new button. Enter the following information. Name Description (Optional) Select the Source Control in the Type drop-down list. Select the credential type in the User Credentials drop-down list and enter the requested information. Username/Password Username Password (Hidden) Create credentials SCM Private Key/Passphrase SCM Private Key Private Key Passphrase (Hidden) Note: Type specific credential information such as keys and passphrases will be hidden or shown as [Encrypted].\nClick Create. Tackle validates the input and creates a new credential. SCM keys must be parsed and checked for validity. If the validation fails, an error message displaying “not a valid key/XML file” is displayed.\nConfiguring Maven credentials Follow the steps below to create new credentials for a Maven repository.\nProcedure\nClick Credentials in the left menu of the Administrator view. Click the Create new button. Enter the following information. Name Description (Optional) Select Maven Settings File in the Type drop-down list. Upload the settings file. Click Create. Tackle validates the input and creates a new credential. Maven settings.xml files must be parsed and checked for validity. If the validation fails, an error message displaying “not a valid key/XML file” is displayed.\nConfiguring proxy credentials Follow the steps below to create new credentials for a proxy repository.\nProcedure\nClick Credentials in the left menu of the Administrator view. Click the Create new button. Enter the following information. Name Description (Optional) Select Proxy in the Type drop-down list. Enter the following information. Username Password Note: Type specific credential information such as keys and passphrases will be hidden or shown as [Encrypted].\nClick Create. Repositories This management module configures the repositories used by Tackle with the following options.\nConfiguring Git repositories Follow the steps below to configure a Git repository.\nClick Repositories and then Git in the left menu of the Administrator view. Click the Consume insecure Git repositories toggle switch to enable. Configuring Subversion repositories Follow the steps below to configure a Subversion repository.\nClick Repositories and then Subversion in the left menu of the Administrator view. Click the Consume insecure Git repositories toggle switch to enable. Subversion Configuring Maven repositories Follow the steps below to configure a Maven repository or clear the local artifact repository.\nConfiguring the repository Click Repositories and then Maven in the left menu of the Administrator view. Click the Force update of dependencies toggle switch to enable. Click the Consume insecure artifact repositories toggle switch to enable. Managing repository size Click Repositories and then Maven in the left menu of the Administrator view. Click the Clear repository link. Note Due to the size of the repository, the size change may not be evident despite the function working properly.\nProxy This management module configures HTTP \u0026amp; HTTPS proxy settings. To configure the proxies click the radio button and enter the following information.\nClick Proxy in the left menu of the Administrator view. Click the HTTP proxy or HTTPS proxy toggle switch to enable the proxy connection. Enter the following information Proxy host Proxy port Click the HTTP proxy credentials or HTTPS proxy credentials toggle switch to enable authentication (optional). Select the credential from the drop-down list. "},{"uri":"http://konveyor.github.io/tackle/tackle2/views/","title":"Tackle 2.0 Views","tags":[],"description":"","content":"The latest version of Tackle has the Developers view and the new Architects view to support the three main roles of users:\nAdministrators: Has access to some application-wide configuration parameters that other users can consume but not change or browse.\nExample actions: Define Git credentials, Maven settings, .xml files.\nArchitects: Often the technical leads for the migration project that can create and modify applications and information related to it. The Architects do not need to have access to sensitive information, but can consume it.\nExample actions: Associate an existing credential to access the repository of a given application.\nMigrators: Developers that can run assessments and analysis, but cannot create or modify applications in the portfolio. Maybe an example action?\nDeveloper view The developer view is intended to be used by migrators and stakeholders and has three pages with different functionalities. Application inventory The Application inventory page manages the applications being migrated and is where the assessment and analysis processes are performed. The application list provides a holistic view of the application portfolio using an extensible tagging model to classify application types. The applications can be expanded to show more detailed information which can be edited and managed.\nThis page has two tabs with different information and functionality: Assessment and Analysis.\nApplication Assessments Use the Assessment tab to facilitate a conversation before migrating an application with stakeholders such as technical subject matter experts and application owners or users. Tackle does not prescribe solutions but instead provides a script of discussion points to identify potential migration risks.\nAssessment categories include:\nApplication details Application dependencies Application architecture Application observability Application cross-cutting concerns Reviewing the results will help stakeholders develop suitable migration strategies for applications or application types. Only one assessment can be done at once, but assessment results can be applied to other applications.\nAdditional functionality includes:\nCopy assessment Copy assessment and review Discard assessment Manage dependencies Manage credentials Delete Analysis Use the Analysis tab to perform an automated examination of the application that views binaries, source code, and dependencies to find possible issues that might prevent the application from running on the new platform. The process starts by retrieving and then analyzing one of the following repositories:\nBinary: Provides a full picture of the application and library dependencies that are embedded into the code. Source code: Only provides a view of the application. Source code + dependencies: Provides a full picture by downloading the source code from the repository, then parses and downloads the dependencies from Maven and adds them to the source code. There is an option to upload a locally stored binary. This only works if a single application is selected.\nAdditional functionality includes:\nManage dependencies Manage credentials Analysis details Cancel analysis Delete Reports The Reports page provides an overview of the assessments and reviews for the entire application inventory. The Reports page contains the following sections:\nCurrent landscape: Displays all applications according to their risk levels. Adoption candidate distribution: Lists the assessed applications with the following columns: Criticality is based on the Business criticality value of the review. Priority is based on the Work priority value of the review. Effort is based on the Effort estimate value of the review. Decision is based on the Proposed action value of the review. By default, all applications are selected. You can clear some of the application check boxes to filter the report.\nSuggested adoption plan: Displays a suggested adoption plan based on effort, priority, and dependencies. Identified risks: Lists the severe risks identified in the assessments for all applications. Controls The Controls page is where the application parameters are managed by the architect or developers as the instance is configured and edited as the project progresses.\nThe parameters include:\nStakeholders Stakeholder groups Job functions Business services Tags (Tag Types) Administrator view The administrator view is intended to be used by administrators to set up the Tackle instance environment. Credentials This management module enables administrators to create and manage credentials for access to private repositories. It also allows for the architects to assign the credentials to applications without knowing their contents. The credentials page displays the available credentials with an Edit and Delete buttons and the following fields:\nName Description Type Source Control Username/Password Source Private Key/Passphrase Maven Settings File Proxy Type specific information Created by Note: Type specific credential information such as keys and passphrases will be hidden or shown as [Encrypted].\nRepositories This management module configures the repositories used by Tackle with the following options. Git Consume insecure Git repositories Subversion Consume insecure Subversion repositories Maven Manage repository size Force update of dependencies Consume insecure artifact repositories Proxy This management module configures HTTP \u0026amp; HTTPS proxy settings and credentials. To configure the proxies click the radio button and enter the following information. Proxy host Proxy port Authentication (optional) Proxy credentials Source\n"},{"uri":"http://konveyor.github.io/tackle/tackle2/instances/","title":"Seeding Instances","tags":[],"description":"","content":"Tackle instances have key parameters that are configured in the Controls window prior to migration by the project architect and can be added and edited as needed.\nThese parameters define applications and individuals, teams, verticals or areas within an organization affected or participating in the migration.\nStakeholders Stakeholder groups Job functions Business services Tag types Tags Seeding Tackle instance The steps to creating and configuring a Tackle instance can be performed in any order, but the suggested order below is the most efficient for creating stakeholders and tags.\nStakeholders\nCreate stakeholder groups Create job functions Create stakeholders Tags\nCreate tag types Create tags Tackle stakeholders and defined by:\nEmail Name Job function Stakeholder groups Creating a new stakeholder group Follow the steps below to create a new stakeholder group. There are no default stakeholder groups defined.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Stakeholder groups tab. Click the Create new button. Enter the following information: Name Description Member(s) Click the Create button. Creating a new job function Tackle uses the job function attribute to classify stakeholders and provides a list of default values that can be expanded. Follow the steps below to create a new job function not included in the default list.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Job functions tab. Click the Create new button. Enter a job function title into the Name text box. Click the Create button. Creating a new stakeholder Follow the steps below to create a new migration project stakeholder.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Stakeholders tab. Click the Create new button. Enter the following information: Email Name Job function: Addition beyond the standard can be created Stakeholder group Click the Create button. Business services Tackle uses the business services attribute to define the departments within the organization that use the application and will be affected by the migration.\nImportant: Business services must be created prior to creating or importing applications.\nCreating a new business service Follow the steps below to create a new business service. There are no default business services defined and must be created prior to creating applications.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Business services tab. Click the Create new button. Enter the following information: Name Description Owner Click the Create button. Creating new tag types Tackle uses tags to classify applications in multiple categories. Follow the steps below to create a new tag type not included in the default list.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Tags tab. Click the Create tag type button. Enter the following information: Name Rank - order the tags appear on applications Color Click the Create button. Creating new tags Follow the steps below to create a new tag not included in the default list.\nProcedure\nClick Controls in the left menu of the Developer view. Click the Tags tab. Click the Create tag types button. Enter the following information: Name Tag Type Click the Create button. Source\n"},{"uri":"http://konveyor.github.io/tackle/installation/","title":"Installing Tackle 2.0","tags":[],"description":"","content":"Follow the steps below to provision the minikube cluster and install Tackle 2.0.\nProvisioning Minikube Follow the steps below to provision minikube for single users deploying Tackle on a workstation. These steps will configure minikube and enable:\nIngress so the Tackle tool can publish outside of the Kubernetes cluster. Operator lifecycle manager (OLM) addon. (OpenShift has OLM installed out of the box but Kubernetes does not.) Procedure\nProvision the minikube cluster with these recommended parameters. Replace \u0026lt;profile name\u0026gt; with your choice of minikube profile name. [user@user ~]$ minikube start --memory=10g -p \u0026lt;profile name\u0026gt; Enable the ingress addon. [user@user ~]$ minikube addons enable ingress -p \u0026lt;profile name\u0026gt; Install Operator Lifecycle Manager (OLM), a tool to help manage the Operators running on your cluster. [user@user ~]$ curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.21.2/install.sh | bash -s v0.21.2 Installing Tackle Operator Operators are a structural layer that manages resources deployed on Kubernetes (database, front end, back end) to automatically create a Tackle instance instead of doing it manually.\nRequirements Tackle requires a total of 5 persistent volumes (PVs) used by different components to successfully deploy, 3 RWO volumes and 2 RWX volumes will be requested via PVCs.\nName Default Size Access Mode Description hub database 5Gi RWO Hub DB hub bucket 100Gi RWX Hub file storage keycloak postgresql 1Gi RWO Keycloak backend DB pathfinder postgresql 1Gi RWO Pathfinder backend DB maven 100Gi RWX maven m2 repository Follow the steps below to install the Tackle Operator in the my-tackle-operator namespace (default) on any Kubernetes distribution, including minikube.\nProcedure\nInstall the Tackle Operator. [user@user ~]$ kubectl create -f https://operatorhub.io/install/tackle-operator.yaml Verify Tackle was installed. [user@user ~]$ kubectl get pods -n my-tackle-operator Repeat this step until konveyor-tackle-XXX and tackle-operator-XXX show 1/1 Running. Create the Tackle instance Follow the steps below to initiate the Tackle instance and set a custom resource (CR) with the tackle_hub.yaml file. CRs can be customized to meet the project needs.\nProcedure\nCreate the instance pointing to the CR file. [user@user ~]$ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - kind: Tackle apiVersion: tackle.konveyor.io/v1alpha1 metadata: name: tackle namespace: my-tackle-operator spec: EOF Verify the instance [user@user ~]$ kubectl get pods -n my-tackle-operator Repeat this step until all components are Completed or Running. Note: This can take one to five minutes depending on the cluster resources.\nLogging in to Tackle Web Console Follow the steps below to log into the Tackle web console.\nProcedure\nAccess the minikube dashboard. This will enable the dashboard add-on, and open the proxy in the default web browser. [user@user ~]$ minikube dashboard -p \u0026lt;profile name\u0026gt; Ensure the top dropdown namespace selector is set to the my-tackle-operator Click Service then Ingresses Click the endpoint IP for the tackle ingress ingress to launch the Tackle web console in a new browser window. Note: This may default to http://$IP_ADDR and fail to load, if so switch to https://$IP_ADDR\nThe default auth enabled credentials are: admin/password Source\n"},{"uri":"http://konveyor.github.io/tackle2/","title":"Welcome to Tackle 2.0","tags":[],"description":"","content":"Tackle documentation\nTackle 2.0 includes the seamless integration with the analysis capabilities of the Windup project, also known downstream as Migration Toolkit for Applications, enabling Tackle to effectively manage, assess and now also analyze applications to have a holistic view at your portfolio when dealing with large scale modernization and Kubernetes adoption initiatives. This is essential to provide key insights that allow Architects leading these massive projects to make informed decisions, thus reducing risks and making the required effort measurable and predictable.\nThe following is a list of the exciting new features included in Tackle 2.0:\nAdministrator perspective: Dedicated perspective to manage tool-wide configuration, with a similar approach and design to the OpenShift Administrator Perspective.. Enhanced RBAC: Three new differentiated personas with different permissions - Administrator, Architect and Migrator Integration with repositories: Full integration with source code (Git, Subversion) and binaries (Maven) repositories to automate the retrieval of applications for analysis. Credentials management: Secure store for multiple credential types (source control, Maven settings files, proxy). Credentials are managed by Administrators and assigned by Architects to applications. Proxy integration: HTTP and HTTPS proxy configuration can be managed in the Tackle UI. Analysis module: Full integration with the Windup project to allow the execution of application analysis from the application inventory. Enhanced analysis modes: Aside from source and binary analysis modes, now Tackle includes the Source + Dependencies mode that parses the POM file available in the source repository to gather dependencies from corporate or public artifact repositories, adding them to the scope of the analysis. Analysis scope selection: Simplified user experience to configure the analysis scope, with the possibility to force the analysis of known Open Source libraries. Authless deployment: Tackle can now be optionally deployed without Keycloak, allowing full unauthenticated admin access to the tool. This is especially useful when deploying the tool in resource constrained environments like local instances of Minikube, where only a single user would have access to it. Seamless upgrades: Tackle lifecycle is now managed by a new operator with Capability Level II, allowing seamless upgrades between GA versions. Find more documentation here or by clicking Tackle 1.0 \u0026amp; 2.0 in the left pane. In case you find anything missing, please let us know by creating an issue in the Konveyor documentation repository.\n"},{"uri":"http://konveyor.github.io/tackle/tackle2/assessanalyze/","title":"Assessing and analyzing applications","tags":[],"description":"","content":"Tackle core functions are assessing and analyzing the applications for migration and are performed on the Application inventory page. Assessing applications Follow the steps below to facilitate discussion of application migration.\nProcedure\nClick Application inventory in the left menu in the Development view. Click the checkbox to the left of the application being assessed. Note: Only one application can be assessed at a time.\nClick the Assess button. Select the Stakeholders and Stakeholder groups from the drop-down lists to track who contributed to the assessment for future reference. Note: Additional list options can be added in the Stakeholder Groups or Stakeholders tab on the Controls page in the Developer view.\nClick the Next button. Click the radio button next to the option that best answers the questions in each category and click Next to go to the next section when complete. Click Save and Review to view the risks that should be taken into account. Applying assessments to other applications Follow the steps below to apply an application assessment to similar applications being migrated.\nProcedure\nClick Application inventory in the left menu in the Development view. Click the checkbox of the application with the completed assessment to copy. Click the menu kebab at the right of the selected application. Select Copy assessment or Copy assessment and review. Click the checkbox of the application(s) to copy the assessment or assessment and review to. Click the Copy button. Running application analysis Follow the steps below to analyze an application for migration.\nProcedure\nClick Application inventory in the left menu in the Development view. Click the Analysis tab. Click the checkbox to the left of the application being analyzed. Note: More than one application can be analyzed at a time.\nCheck the credentials assigned to the application. Click the Analyze button. Select source for analysis from the drop-down list. Click the Next button. Set the target to one or more of the transformation targets. Application server migration to… Containerization Quarkus OpenJDK OpenJDK 11 Linux Camel Click the Next button. Click a radio button to select one of the following scope options to narrow down and focus the analysis. Application and internal dependencies only Application and all dependencies, including known Open Source libraries Select the list of packages to be analyzed manually (Type the file name and click Add.) Exclude packages (Type the package name and click Add.) Click the Next button. Set custom rules by typing a name or searching and clicking the Add Rules button. Consider the following rules: Target Source Exclude tags: Rules with these tags are not processed. Note: Analysis engines use standard rules for a comprehensive set of migration targets, but if the target is not included or customized frameworks custom rules can be added. Custom rules files will be validated.\nClick the Next button. Add or remove targets and sources to narrow the analysis. Exclude rules tags as necessary. Click the Next button. Verify the analysis parameters. Click the Run button. Analysis status will show Scheduled as it downloads the image for the container to execute. When that is complete, it will show In-progress until complete.\nNote: The analysis will take minutes to hours to run depending on the size of the application and the cluster capacity and resources.\nTackle relies on Kubernetes scheduling capabilities to determine how many analyzer instances are created based on cluster capacity. If several applications are selected for analysis, by default, only one analyzer can be provisioned at a time. With more cluster capacity, more analysis processes can be executed in parallel. 19. Expand the application and click the Report link to the right of Analysis when completed.\nReviewing the Analysis Report Follow the steps below to review the analysis report. For more information see Chapter 3. Reviewing the reports of the CLI Guide Migration Toolkit For Applications 5.3.\nProcedure\nClick Application inventory in the left menu in the Development view. Expand the application with a completed analysis. Click the Report link. Click the dependencies or source links. Click the tabs to review the report. Source\n"},{"uri":"http://konveyor.github.io/tackle/tackle2/addapps/","title":"Adding applications","tags":[],"description":"","content":"Applications can be added to Tackle by creating new applications from scratch manually or by importing them. Tackle applications are defined by manually entered and pre-defined attributes:\nName (manual) Description (manual) Business service (pre-defined) Tags (pre-defined) Source code Binary Creating a new application Follow the steps below to add a new application to the inventory for assessment and analysis.\nNote: Before starting this procedure, it is helpful to set up business services and check tags and tag types and create additions as needed.\nProcedure\nClick Application inventory in the left menu of the Developer view. Click the Create new button. Enter the following information: Name Description Business service Tags Comments Click the arrow to the right of Source Code to expand the section. Enter the following information: Repository type Source Repository Branch Root path Click the down arrow to the right of Binary to expand the section. Enter the following information: Group Artifact Version Packaging Click the Create button. Assigning application credentials Follow the steps below to assign credentials to one or more applications.\nProcedure\nClick Application inventory in the left menu of the Developer view. Click the Analysis tab. Click the menu kebab to the right of the Review button and select Manage credentials. Select the credential from the Source credentials or Maven setting drop-down list. Click the Save button. Importing application lists Tackle allows users to import a spreadsheet of existing applications instead of entering them manually. A recommended sample CSV template has been made available to users to fill in with the required information.\nDownloading the application list CSV template. Follow the steps below to download the sample CSV template.\nProcedure\nClick Application inventory in the left menu of the Developer view. Click the menu kebab button to the right of the Review button. Click Manage imports to open the Application imports page. Click the kebab menu to the right of the Import button and click Download CSV template. Importing an application list Follow the steps below to import a .csv file provided by the customer containing a list of applications and their attributes. Importing can be performed by using the Import or Manage import functions, but using the steps below is recommended for verifying the import was successful.\nNote: Importing will only add applications, it will not overwrite any existing ones.\nProcedure\nReview the import file for all the required information in the required format. Click Application inventory in the left menu of the Developer view. Click the menu kebab button to the right of the Review button. Click the Import button. Browse to the file and click the Import button. Verify the import has completed and check the number of rows accepted or rejected. Review the imported applications by clicking the arrow to the left of the check box to expand. Important: Rows accepted may not match the number of applications in the Application inventory list because some rows are dependencies. To verify, check the record type column of the CSV file for applications defined as 1 and dependencies as 2.\nSource\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/migrateoptions/","title":"Migration plan options","tags":[],"description":"","content":"On the Migration plans page of the Forklift web console, click the Options menu kebab beside a migration plan to access the following options:\nEdit: Edit the details of a migration plan. A migration plan cannot be edited while it is running or after it has completed successfully. Duplicate: Create a new migration plan with the same virtual machines (VMs), parameters, mappings, and hooks as an existing plan. Use this feature for the following tasks: Migrating VMs to a different namespace. Editing an archived migration plan. Editing a migration plan with a different status, for example, failed, canceled, running, critical, or ready. Archive: Delete the logs, history, and metadata of a migration plan. The plan cannot be edited or restarted. It can only be viewed. Important: The Archive option is irreversible, but an archived plan can be duplicated.\nDelete: Permanently remove a migration plan. A running migration plan cannot be deleted. Important The Delete option is irreversible.\nDeleting a migration plan does not remove temporary resources such as importer pods, conversion pods, config maps, secrets, failed VMs, and data volumes. (BZ#2018974) A migration plan must be archived before deleting it in order to clean up the temporary resources.\nView details: Display the details of a migration plan. Restart: Restart a failed or canceled migration plan. Cancel scheduled cutover: Cancel a scheduled cutover migration for a warm migration plan. Source\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/migratecli/","title":"Migrating virtual machines using the command line","tags":[],"description":"","content":"Follow the steps below to migrate virtual machines (VMs) to KubeVirt using the command line (CLI) by creating Forklift custom resources (CRs) and specifying:\nA name for cluster-scoped CRs A name and a namespace for namespace-scoped CRs Prerequisites\nMust be logged in as a user with cluster-admin privileges. VMware only: Must have the vCenter SHA-1 fingerprint. Must create a VMware Virtual Disk Development Kit (VDDK) image in a secure registry that is accessible to all clusters. Procedure\nCreate a Secret manifest for the source provider credentials: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: \u0026lt;secret\u0026gt; namespace: konveyor-forklift type: Opaque stringData: user: \u0026lt;user\u0026gt; (1) password: \u0026lt;password\u0026gt; (2) cacert: | (3) \u0026lt;oVirt_ca_certificate\u0026gt; thumbprint: \u0026lt;vcenter_fingerprint\u0026gt; (4) EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the vCenter user or the oVirt Engine user. (2) Specify the user password. (3) oVirt only: Specify the CA certificate of the Engine. Retrieve it at https://\u0026lt;{rhv-short}_engine_host\u0026gt;/ovirt-engine/services/pki-resource?resource=ca-certificate\u0026amp;format=X509-PEM-CA. (4) VMware only: Specify the vCenter SHA-1 fingerprint. Create a Provider manifest for the source provider: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Provider metadata: name: \u0026lt;provider\u0026gt; namespace: konveyor-forklift spec: type: \u0026lt;provider_type\u0026gt; (1) url: \u0026lt;api_end_point\u0026gt; (2) settings: vddkInitImage: \u0026lt;registry_route_or_server_path\u0026gt;/vddk: (3) secret: name: \u0026lt;secret\u0026gt; (4) namespace: konveyor-forklift EOF The explanations below refer to the callouts in the sample code above.\n(1) Allowed values are ovirt and vsphere. (2) Specify the API end point URL, for example, https://\u0026lt;vCenter_host\u0026gt;/sdk for vSphere or https://\u0026lt;{rhv-short}_engine_host\u0026gt;/ovirt-engine/api/ for oVirt. (3) VMware only: Specify the VDDK image created. (4) Specify the name of provider Secret CR. VMware only: Create a Host manifest: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Host metadata: name: \u0026lt;vmware_host\u0026gt; namespace: konveyor-forklift spec: provider: namespace: konveyor-forklift name: \u0026lt;source_provider\u0026gt; (1) id: \u0026lt;source_host_mor\u0026gt; (2) ipAddress: \u0026lt;source_network_ip\u0026gt; (3) EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the name of the VMware Provider CR. (2) Specify the managed object reference (MOR) of the VMware host. (3) Specify the IP address of the VMware migration network. Create a NetworkMap manifest to map the source and destination networks: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: NetworkMap metadata: name: \u0026lt;network_map\u0026gt; namespace: konveyor-forklift spec: map: - destination: name: \u0026lt;pod\u0026gt; namespace: konveyor-forklift type: pod (1) source: (2) id: \u0026lt;source_network_id\u0026gt; (3) name: \u0026lt;source_network_name\u0026gt; - destination: name: \u0026lt;network_attachment_definition\u0026gt; (4) namespace: \u0026lt;network_attachment_definition_namespace\u0026gt; (5) type: multus source: id: \u0026lt;source_network_id\u0026gt; name: \u0026lt;source_network_name\u0026gt; provider: source: name: \u0026lt;source_provider\u0026gt; namespace: konveyor-forklift destination: name: \u0026lt;destination_cluster\u0026gt; namespace: konveyor-forklift EOF The explanations below refer to the callouts in the sample code above.\n(1) Allowed values are pod and multus. (2) Use either the id or the name parameter to specify the source network. (3) Specify the VMware network MOR or oVirt network UUID. (4) Specify a network attachment definition for each additional KubeVirt network. (5) Specify the namespace of the KubeVirt network attachment definition. Create a StorageMap manifest to map source and destination storage: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: StorageMap metadata: name: \u0026lt;storage_map\u0026gt; namespace: konveyor-forklift spec: map: - destination: storageClass: \u0026lt;storage_class\u0026gt; accessMode: \u0026lt;access_mode\u0026gt; (1) source: id: \u0026lt;source_datastore\u0026gt; (2) - destination: storageClass: \u0026lt;storage_class\u0026gt; accessMode: \u0026lt;access_mode\u0026gt; source: id: \u0026lt;source_datastore\u0026gt; provider: source: name: \u0026lt;source_provider\u0026gt; namespace: konveyor-forklift destination: name: \u0026lt;destination_cluster\u0026gt; namespace: konveyor-forklift EOF The explanations below refer to the callouts in the sample code above.\n(1) Allowed values are ReadWriteOnce and ReadWriteMany. (2) Specify the VMware data storage MOR or oVirt storage domain UUID, for example, f2737930-b567-451a-9ceb-2887f6207009. Optional: Create a Hook manifest to run custom code on a VM during the phase specified in the Plan CR: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Hook metadata: name: \u0026lt;hook\u0026gt; namespace: konveyor-forklift spec: image: quay.io/konveyor/hook-runner (1) playbook: | (2) LS0tCi0gbmFtZTogTWFpbgogIGhvc3RzOiBsb2NhbGhvc3QKICB0YXNrczoKICAtIG5hbWU6IExv YWQgUGxhbgogICAgaW5jbHVkZV92YXJzOgogICAgICBmaWxlOiAiL3RtcC9ob29rL3BsYW4ueW1s IgogICAgICBuYW1lOiBwbGFuCiAgLSBuYW1lOiBMb2FkIFdvcmtsb2FkCiAgICBpbmNsdWRlX3Zh cnM6CiAgICAgIGZpbGU6ICIvdG1wL2hvb2svd29ya2xvYWQueW1sIgogICAgICBuYW1lOiB3b3Jr bG9hZAoK EOF The explanations below refer to the callouts in the sample code above.\n(1) Use the default hook-runner image or specify a custom image. A playbook does not need to be specified, if a custom image is. (2) Optional: Base64-encoded Ansible playbook. The image must be hook-runner if a playbook is specified. Create a Plan manifest for the migration: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Plan metadata: name: \u0026lt;plan\u0026gt; (1) namespace: konveyor-forklift spec: warm: true (2) provider: source: name: \u0026lt;source_provider\u0026gt; namespace: konveyor-forklift destination: name: \u0026lt;destination_cluster\u0026gt; namespace: konveyor-forklift map: network: (3) name: \u0026lt;network_map\u0026gt; (4) namespace: konveyor-forklift storage: name: \u0026lt;storage_map\u0026gt; (5) namespace: konveyor-forklift targetNamespace: konveyor-forklift vms: (6) - id: \u0026lt;source_vm\u0026gt; (7) - name: \u0026lt;source_vm\u0026gt; hooks: (8) - hook: namespace: konveyor-forklift name: \u0026lt;hook\u0026gt; (9) step: \u0026lt;step\u0026gt; (10) EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the name of the Plan CR. (2) Specify whether the migration is warm or cold. Only the precopy stage will run in a warm migration if the cutover parameter in the Migration manifest value is not specified. (3) Add multiple network mappings if needed. (4) Specify the name of the NetworkMap CR. (5) Specify the name of the StorageMap CR. (6) Use either the id or the name parameter to specify the source VMs. (7) Specify the VMware VM MOR or oVirt VM UUID. (8) Optional: Specify up to two hooks for a VM. Each hook must run during a separate migration step. (9) Specify the name of the Hook CR. (10) Allowed values are PreHook, before the migation plan starts, or PostHook, after the migration is complete. Create a Migration manifest to run the Plan CR: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Migration metadata: name: \u0026lt;migration\u0026gt; (1) namespace: konveyor-forklift spec: plan: name: \u0026lt;plan\u0026gt; (2) namespace: konveyor-forklift cutover: \u0026lt;cutover_time\u0026gt; (3) EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the name of the Migration CR. (2) Specify the name of the Plan CR being run. The Migration CR creates a VirtualMachine CR for each VM that is migrated. (3) Optional: Specify a cutover time according to the ISO 8601 format with the UTC time offset, for example, 2021-04-04T01:23:45.678+09:00. Multiple Migration CRs can be associated with a single Plan CR. If a migration does not complete, create a new Migration CR, without changing the Plan CR, to migrate the remaining VMs. Retrieve the Migration CR to monitor the progress of the migration: $ kubectl get migration/\u0026lt;migration\u0026gt; -n konveyor-forklift -o yaml Obtaining the SHA-1 fingerprint of a vCenter host Obtain the SHA-1 fingerprint of a vCenter host in order to create a Secret CR.\nProcedure\nRun the following command: $ openssl s_client \\ -connect \u0026lt;vcenter_host\u0026gt;:443 \\ (1) \u0026lt; /dev/null 2\u0026gt;/dev/null \\ | openssl x509 -fingerprint -noout -in /dev/stdin \\ | cut -d \u0026#39;=\u0026#39; -f 2 The explanations below refer to the callouts in the sample code above.\n(1) Specify the IP address or FQDN of the vCenter host. Example output:\n01:23:45:67:89:AB:CD:EF:01:23:45:67:89:AB:CD:EF:01:23:45:67 Canceling a migration Forklift functionality cancel cancel an entire migration or individual virtual machines (VMs) while a migration is in progress from the command line interface (CLI).\nCanceling an entire migration Follow the steps below to cancel an entire migration.\nProcedure\nDelete the Migration CR: $ kubectl delete migration \u0026lt;migration\u0026gt; -n konveyor-forklift (1) The explanations below refer to the callouts in the sample code above.\n(1) Specify the name of the Migration CR. Canceling individual VMs migration Follow the steps below to cancel migrating an individual VM.\nProcedure\nAdd the individual VMs to the spec.cancel block of the Migration manifest: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Migration metadata: name: \u0026lt;migration\u0026gt; namespace: konveyor-forklift ... spec: cancel: - id: vm-102 (1) - id: vm-203 - name: rhel8-vm EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify a VM by using the id key or the name key. The value of the id key is the managed object reference, for a VMware VM, or the VM UUID, for a oVirt VM. Retrieve the Migration CR to monitor the progress of the remaining VMs: $ kubectl get migration/\u0026lt;migration\u0026gt; -n konveyor-forklift -o yaml Source\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/migrateweb/","title":"Migrating virtual machines using the web console","tags":[],"description":"","content":"Migrate virtual machines (VMs) to KubeVirt using the Forklift web console by performing the folloing tasks:\nAdding source and destination providers Creating network and storage mapping Creating and running a migration plan Prerequisites\nEnsure that all Prerequisites are set. VMware only: Create a VMware Virtual Disk Development Kit (VDDK) image. Adding source providers Follow the procedures below to add VMware or oVirt source providers using the Forklift web console.\nAdding a VMware source provider Add a VMware source provider by using the Forklift web console.\nPrerequisites\nVMware Virtual Disk Development Kit (VDDK) image in a secure registry that is accessible to all clusters. Procedure\nOpen the Forklift web console. Click Providers, then Add provider. Select VMware from the Type list. Fill in the following fields: Name: Name to display in the list of providers Hostname or IP address: vCenter host name or IP address Username: vCenter user, for example, user@vsphere.local Password: vCenter user password VDDK init image: VDDKInitImage path Click Verify certificate. Select the I trust the authenticity of this certificate checkbox. Click Add to add and save the provider. The source provider appears in the list of providers.\nAdding an oVirt source provider Add an oVirt source provider by using the Forklift web console.\nPrerequisites\nCA certificate of the Engine. Procedure\nOpen the Forklift web console. Click Providers, then Add provider. Select Red Hat Virtualization from the Type list. Fill in the following fields: Name: Name toU display in the list of providers Hostname or IP address: Engine host name or IP address Username: Engine user Password: Engine password CA certificate: CA certificate of the Engine Click Add to add and save the provider. The source provider appears in the list of providers.\nSelecting a migration network for a source provider Select a migration network in the Forklift web console for a source provider to reduce risk to the source environment and to improve performance.\nImportant: Using the default network for migration can result in poor performance because the network might not have sufficient bandwidth which can have a negative effect on the source platform because the disk transfer operation might saturate the network.\nPrerequisites\nThe migration network must a minimum speed of 10 Gbps for disk transfer. The migration network must be accessible to the KubeVirt nodes through the default gateway. The source virtual disks are copied by a pod that is connected to the pod network of the target namespace. The migration network must have jumbo frames enabled. Procedure\nOpen the Forklift web console. Click Providers, the the oVirt8 or VMware tab. Click the host number in the Hosts column beside a provider to view a list of hosts. Select one or more hosts and click Select migration network. Select a Network. Note: Clear the selection by selecting the default network.\nComplete the following fields: For VMware:\nESXi host admin username: Specify the ESXi host admin user, for example, root. ESXi host admin password: Specify the ESXi host admin password. For oVirt:\nUsername: Specify the Engine user. Password: Specify the Engine password. Click Save. Verify that the status of each host is Ready. Note: If a host status is not Ready, the host might be unreachable on the migration network or the credentials might be incorrect. Modify the host configuration and save the changes to correct.\nAdding a KubeVirt destination provider Add a KubeVirt destination provider to the Forklift web console in addition to the default KubeVirt destination provider, which is the provider where Forklift.\nPrerequisites\nKubeVirt service account token with cluster-admin privileges. Procedure\nOpen the Forklift web console. Click Providers, the Add provider. Select KubeVirt from the Type list. Complete the following fields: Cluster name: Specify the cluster name to display in the list of target providers. URL: Specify the API endpoint of the cluster. Service account token: Specify the cluster-admin service account token. Click Check connection to verify the credentials. Click Add. The provider appears in the list of providers.\nSelecting a migration network for a KubeVirt provider Select a default migration network for a KubeVirt provider in the Forklift web console to improve performance. The default migration network is used to transfer disks to the namespaces in which it is configured.\nNote: If a migration network is not selected, the default migration network is the pod network which might not be optimal for disk transfer.\nOverride the default migration network of the provider by selecting a different network when creating a migration plan.\nProcedure\nOpen the Forklift web console. Click Providers then the KubeVirt tab. Select a provider and click Select migration network. Select a network from the list of available networks and click Select. Click the network number in the Networks column beside the provider to verify that the selected network is the default migration network. Creating a network mapping Create one or more network mappings using the Forklift web console to map source networks to KubeVirt networks.\nPrerequisites\nSource and target providers added to the web console. Note: If more than one source and target network are mapped, each additional KubeVirt network requires its own network attachment definition.\nProcedure\nClick Mappings. Click the Network tab then click Create mapping. Complete the following fields: Name: Enter a name to display in the network mappings list. Source provider: Select a source provider. Target provider: Select a target provider. Source networks: Select a source network. Target namespaces/networks: Select a target network. Optional: Click Add to create additional network mappings or to map multiple source networks to a single target network. Note: If an additional network mapping is created, select the network attachment definition as the target network.\nClick Create. The network mapping is displayed on the Network mappings screen.\nCreating a storage mapping Create a storage mapping using the Forklift web console to map source data stores to KubeVirt storage classes.\nPrerequisites\nSource and target providers added to the web console. Local and shared persistent storage that support VM migration. Procedure\nClick Mappings. Click the Storage tab and then click Create mapping. Set the following parameters: Name of the storage mapping. Source provider Target provider VMware: Source datastore Target storage class oVirt: Source storage domain Target storage class Optional: Click Add to create additional storage mappings or to map multiple source data stores or storage domains to a single storage class. Click Create. The mapping is displayed on the Storage mappings page.\nCreating a migration plan Create a migration plan by using the Forklift web console.\nA migration plan allows virtual machines to be grouped and migrated together or with the same migration parameters. For example: a percentage of the members of a cluster or a complete application.\nConfigure a hook to run an Ansible playbook or custom container image during a specified stage of the migration plan.\nPrerequisites\nIf Forklift is not installed on the target cluster, add a target provider on the Providers page of the web console. Procedure\nOpen the web console. Click Migration plans then click Create migration plan. Complete the following fields: Plan name: Enter a migration plan name to display in the migration plan list. Plan description: Optional: Brief description of the migration plan. Source provider: Select a source provider. Target provider: Select a target provider. Target namespace: Type to search for an existing target namespace or create a new namespace. Change the migration transfer network for this plan by clicking Select a different network, selecting a network from the list, and clicking Select. If a migration transfer network is defined for the KubeVirt provider and if the network is in the target namespace, that network is the default network for all migration plans. Otherwise, the pod network is used.\nClick Next. Select options to filter the list of source VMs and click Next. Select the VMs to migrate and then click Next. Select an existing network mapping or create a new network mapping. Creating a new network mapping Follow the steps below to create a network mapping.\nProcedure\nSelect a target network for each source network. Optional: Select Save mapping to use again and enter a network mapping name. Click Next. Select an existing storage mapping or create a new storage mapping. Creating a new storage mapping Follow the steps below to create a storage mapping.\nProcedure\nSelect a target storage class for each VMware data store or oVirt storage domain. Optional: Select Save mapping to use again and enter a storage mapping name. Click Next. Select a migration type and click Next. Cold migration: The source VMs are stopped while the data is copied. Warm migration: The source VMs run while the data is copied incrementally and the cutover runs later which stops the VMs and copies the remaining VM data and metadata. Optional: Create a migration hook to run an Ansible playbook before or after migration: Click Add hook. Select the step when the hook will run. Select a hook definition: Ansible playbook: Browse to the Ansible playbook or paste it into the field. Custom container image: Enter the image path: \u0026lt;registry_path\u0026gt;/\u0026lt;image_name\u0026gt;: to not use the default hook-runner image. Important: The registry must be accessible to the OKD cluster.\nClick Next. Review the migration plan and click Finish. The migration plan is saved in the migration plan list.\nClick the Options menu kebab of the migration plan and select View details to verify the migration plan details. Running a migration plan Run a migration plan and view its progress in the Forklift web console.\nPrerequisites\nValid migration plan. Procedure\nClick Migration plans. The Migration plans list displays the source and target providers, the number of virtual machines (VMs) being migrated, and the status of the plan.\nClick Start beside a migration plan to start the migration. Warm migration only: When the precopy stage starts.\nClick Cutover to complete the migration. Expand a migration plan to view the migration details. The migration details screen displays the migration start and end time, the amount of data copied, and a progress pipeline for each VM being migrated.\nExpand a VM to view the migration steps, elapsed time of each step, and its state. Source\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/cancelmigrate/","title":"Canceling a migration","tags":[],"description":"","content":"Follow the steps below to cancel the migration of some or all virtual machines (VMs) while a migration plan is in progress by using the Forklift web console.\nProcedure\nClick Migration Plans. Click the name of a running migration plan to view the migration details. Select one or more VMs and click Cancel. Click Yes, cancel to confirm the cancellation. The status of the VM shows canceled in the Migration details by VM list. The unmigrated and the migrated virtual machines are not affected.\nRestarting Restart a canceled migration by clicking Restart beside the migration plan on the Migration plans page.\nSource\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/advancedmigrate/","title":"Advanced migration options","tags":[],"description":"","content":"Changing precopy intervals for warm migration Follow the steps below to change the snapshot interval by patching the ForkliftController custom resource (CR).\nProcedure\nPatch the ForkliftController CR: $ kubectl patch forkliftcontroller/\u0026lt;forklift-controller\u0026gt; -n konveyor-forklift -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;controller_precopy_interval\u0026#34;: \u0026lt;60\u0026gt;}}\u0026#39; --type=merge (1) The explanation below refers to the callout in the sample code above.\n(1) Specify the precopy interval in minutes. The default value is 60. Note: The forklift-controller pod does not need to be restarted.\nCreating custom rules for the Validation service The Validation service uses Open Policy Agent (OPA) policy rules to check the suitability of each virtual machine (VM) for migration. The Validation service generates a list of concerns for each VM, which are stored in the Provider Inventory service as VM attributes. The web console displays the concerns for each VM in the provider inventory.\nCustom rules to extend the default ruleset of the Validation service can be created. For example, create a rule that checks whether a VM has multiple disks.\nAbout Rego files Validation rules are written in Rego, the Open Policy Agent (OPA) native query language. The rules are stored as .rego files in the /usr/share/opa/policies/io/konveyor/forklift/ directory of the Validation pod.\nEach validation rule is defined in a separate .rego file and tests for a specific condition. If the condition evaluates as true, the rule adds a {“category”, “label”, “assessment”} hash to the concerns. The concerns content is added to the concerns key in the inventory record of the VM. The web console displays the content of the concerns key for each VM in the provider inventory.\nThe following .rego file example checks for distributed resource scheduling enabled in the cluster of a VMware VM:\ndrs_enabled.rego example:\npackage io.konveyor.forklift.vmware (1) has_drs_enabled { input.host.cluster.drsEnabled (2) } concerns[flag] { has_drs_enabled flag := { \u0026#34;category\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;VM running in a DRS-enabled cluster\u0026#34;, \u0026#34;assessment\u0026#34;: \u0026#34;Distributed resource scheduling is not currently supported by OpenShift Virtualization. The VM can be migrated but it will not have this feature in the target environment.\u0026#34; } } The explanations below refer to the callouts in the sample code above.\n(1) Each validation rule is defined within a package. The package namespaces are io.konveyor.forklift.vmware for VMware and io.konveyor.forklift.ovirt for oVirt. (2) Query parameters are based on the input key of the Validation service JSON. Checking the default validation rules Before creating a custom rule, follow the steps below to check the default rules of the Validation service to prevent creating a rule that redefines an existing default value.\nExample: If a default rule contains the line default valid_input = false and a custom rule that contains the line default valid_input = true is created, the Validation service will not start.\nProcedure\nConnect to the terminal of the Validation pod: $ kubectl rsh \u0026lt;validation_pod\u0026gt; Go to the OPA policies directory for the provider: $ cd /usr/share/opa/policies/io/konveyor/forklift/\u0026lt;provider\u0026gt; (1) The explanations below refer to the callouts in the sample code above.\n(1) Specify vmware or ovirt. Search for the default policies: $ grep -R \u0026#34;default\u0026#34; * Retrieving the Inventory service JSON Follow the steps below to retrieve the Inventory service JSON by sending an Inventory service query to a virtual machine (VM). The output contains an \u0026ldquo;input\u0026rdquo; key, which contains the inventory attributes that are queried by the Validation service rules.\nCreate a validation rule based on any attribute in the \u0026ldquo;input\u0026rdquo; key, for example, input.snapshot.kind.\nProcedure\nRetrieve the Inventory service route: $ kubectl get route \u0026lt;inventory_service\u0026gt; -n konveyor-forklift Retrieve the UUID of a provider: $ GET https://\u0026lt;inventory_service_route\u0026gt;/providers/\u0026lt;provider\u0026gt; (1) The explanations below refer to the callouts in the sample code above.\n(1) Allowed values for the provider are vsphere and ovirt. Retrieve the VMs of a provider: $ GET https://\u0026lt;inventory_service_route\u0026gt;/providers/\u0026lt;provider\u0026gt;/\u0026lt;UUID\u0026gt;/vms Retrieve the details of a VM: $ GET https://\u0026lt;inventory_service_route\u0026gt;/providers/\u0026lt;provider\u0026gt;/\u0026lt;UUID\u0026gt;/workloads/\u0026lt;vm\u0026gt; Example output:\n{ \u0026#34;input\u0026#34;: { \u0026#34;selfLink\u0026#34;: \u0026#34;providers/vsphere/c872d364-d62b-46f0-bd42-16799f40324e/workloads/vm-431\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;vm-431\u0026#34;, \u0026#34;parent\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Folder\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;group-v22\u0026#34; }, \u0026#34;revision\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;iscsi-target\u0026#34;, \u0026#34;revisionValidated\u0026#34;: 1, \u0026#34;isTemplate\u0026#34;: false, \u0026#34;networks\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-31\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-33\u0026#34; } ], \u0026#34;disks\u0026#34;: [ { \u0026#34;key\u0026#34;: 2000, \u0026#34;file\u0026#34;: \u0026#34;[iSCSI_Datastore] iscsi-target/iscsi-target-000001.vmdk\u0026#34;, \u0026#34;datastore\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-63\u0026#34; }, \u0026#34;capacity\u0026#34;: 17179869184, \u0026#34;shared\u0026#34;: false, \u0026#34;rdm\u0026#34;: false }, { \u0026#34;key\u0026#34;: 2001, \u0026#34;file\u0026#34;: \u0026#34;[iSCSI_Datastore] iscsi-target/iscsi-target_1-000001.vmdk\u0026#34;, \u0026#34;datastore\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-63\u0026#34; }, \u0026#34;capacity\u0026#34;: 10737418240, \u0026#34;shared\u0026#34;: false, \u0026#34;rdm\u0026#34;: false } ], \u0026#34;concerns\u0026#34;: [], \u0026#34;policyVersion\u0026#34;: 5, \u0026#34;uuid\u0026#34;: \u0026#34;42256329-8c3a-2a82-54fd-01d845a8bf49\u0026#34;, \u0026#34;firmware\u0026#34;: \u0026#34;bios\u0026#34;, \u0026#34;powerState\u0026#34;: \u0026#34;poweredOn\u0026#34;, \u0026#34;connectionState\u0026#34;: \u0026#34;connected\u0026#34;, \u0026#34;snapshot\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;VirtualMachineSnapshot\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;snapshot-3034\u0026#34; }, \u0026#34;changeTrackingEnabled\u0026#34;: false, \u0026#34;cpuAffinity\u0026#34;: [ 0, 2 ], \u0026#34;cpuHotAddEnabled\u0026#34;: true, \u0026#34;cpuHotRemoveEnabled\u0026#34;: false, \u0026#34;memoryHotAddEnabled\u0026#34;: false, \u0026#34;faultToleranceEnabled\u0026#34;: false, \u0026#34;cpuCount\u0026#34;: 2, \u0026#34;coresPerSocket\u0026#34;: 1, \u0026#34;memoryMB\u0026#34;: 2048, \u0026#34;guestName\u0026#34;: \u0026#34;Red Hat Enterprise Linux 7 (64-bit)\u0026#34;, \u0026#34;balloonedMemory\u0026#34;: 0, \u0026#34;ipAddress\u0026#34;: \u0026#34;10.19.2.96\u0026#34;, \u0026#34;storageUsed\u0026#34;: 30436770129, \u0026#34;numaNodeAffinity\u0026#34;: [ \u0026#34;0\u0026#34;, \u0026#34;1\u0026#34; ], \u0026#34;devices\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;RealUSBController\u0026#34; } ], \u0026#34;host\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;host-29\u0026#34;, \u0026#34;parent\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Cluster\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;domain-c26\u0026#34; }, \u0026#34;revision\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;IP address or host name of the vCenter host or {rhv-short} Engine host\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;providers/vsphere/c872d364-d62b-46f0-bd42-16799f40324e/hosts/host-29\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;inMaintenance\u0026#34;: false, \u0026#34;managementServerIp\u0026#34;: \u0026#34;10.19.2.96\u0026#34;, \u0026#34;thumbprint\u0026#34;: \u0026lt;thumbprint\u0026gt;, \u0026#34;timezone\u0026#34;: \u0026#34;UTC\u0026#34;, \u0026#34;cpuSockets\u0026#34;: 2, \u0026#34;cpuCores\u0026#34;: 16, \u0026#34;productName\u0026#34;: \u0026#34;VMware ESXi\u0026#34;, \u0026#34;productVersion\u0026#34;: \u0026#34;6.5.0\u0026#34;, \u0026#34;networking\u0026#34;: { \u0026#34;pNICs\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PhysicalNic-vmnic0\u0026#34;, \u0026#34;linkSpeed\u0026#34;: 10000 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PhysicalNic-vmnic1\u0026#34;, \u0026#34;linkSpeed\u0026#34;: 10000 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PhysicalNic-vmnic2\u0026#34;, \u0026#34;linkSpeed\u0026#34;: 10000 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PhysicalNic-vmnic3\u0026#34;, \u0026#34;linkSpeed\u0026#34;: 10000 } ], \u0026#34;vNICs\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk2\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;VM_Migration\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;192.168.79.13\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, \u0026#34;mtu\u0026#34;: 9000 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk0\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;Management Network\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;10.19.2.13\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.255.128\u0026#34;, \u0026#34;mtu\u0026#34;: 1500 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk1\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;Storage Network\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;172.31.2.13\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.0.0\u0026#34;, \u0026#34;mtu\u0026#34;: 1500 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk3\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;dvportgroup-48\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;192.168.61.13\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, \u0026#34;mtu\u0026#34;: 1500 }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualNic-vmk4\u0026#34;, \u0026#34;portGroup\u0026#34;: \u0026#34;VM_DHCP_Network\u0026#34;, \u0026#34;dPortGroup\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ipAddress\u0026#34;: \u0026#34;10.19.2.231\u0026#34;, \u0026#34;subnetMask\u0026#34;: \u0026#34;255.255.255.128\u0026#34;, \u0026#34;mtu\u0026#34;: 1500 } ], \u0026#34;portGroups\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch0\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-Management Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Management Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch0\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_10G_Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_10G_Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_Storage\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_Storage\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_DHCP_Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_DHCP_Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-Storage Network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Storage Network\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_Isolated_67\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_Isolated_67\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch2\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.PortGroup-VM_Migration\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;VM_Migration\u0026#34;, \u0026#34;vSwitch\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch2\u0026#34; } ], \u0026#34;switches\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vSwitch0\u0026#34;, \u0026#34;portGroups\u0026#34;: [ \u0026#34;key-vim.host.PortGroup-VM Network\u0026#34;, \u0026#34;key-vim.host.PortGroup-Management Network\u0026#34; ], \u0026#34;pNICs\u0026#34;: [ \u0026#34;key-vim.host.PhysicalNic-vmnic4\u0026#34; ] }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vSwitch1\u0026#34;, \u0026#34;portGroups\u0026#34;: [ \u0026#34;key-vim.host.PortGroup-VM_10G_Network\u0026#34;, \u0026#34;key-vim.host.PortGroup-VM_Storage\u0026#34;, \u0026#34;key-vim.host.PortGroup-VM_DHCP_Network\u0026#34;, \u0026#34;key-vim.host.PortGroup-Storage Network\u0026#34; ], \u0026#34;pNICs\u0026#34;: [ \u0026#34;key-vim.host.PhysicalNic-vmnic2\u0026#34;, \u0026#34;key-vim.host.PhysicalNic-vmnic0\u0026#34; ] }, { \u0026#34;key\u0026#34;: \u0026#34;key-vim.host.VirtualSwitch-vSwitch2\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vSwitch2\u0026#34;, \u0026#34;portGroups\u0026#34;: [ \u0026#34;key-vim.host.PortGroup-VM_Isolated_67\u0026#34;, \u0026#34;key-vim.host.PortGroup-VM_Migration\u0026#34; ], \u0026#34;pNICs\u0026#34;: [ \u0026#34;key-vim.host.PhysicalNic-vmnic3\u0026#34;, \u0026#34;key-vim.host.PhysicalNic-vmnic1\u0026#34; ] } ] }, \u0026#34;networks\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-31\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-34\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-57\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-33\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;dvportgroup-47\u0026#34; } ], \u0026#34;datastores\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-35\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-63\u0026#34; } ], \u0026#34;vms\u0026#34;: null, \u0026#34;networkAdapters\u0026#34;: [], \u0026#34;cluster\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;domain-c26\u0026#34;, \u0026#34;parent\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Folder\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;group-h23\u0026#34; }, \u0026#34;revision\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;mycluster\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;providers/vsphere/c872d364-d62b-46f0-bd42-16799f40324e/clusters/domain-c26\u0026#34;, \u0026#34;folder\u0026#34;: \u0026#34;group-h23\u0026#34;, \u0026#34;networks\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-31\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-34\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-57\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;network-33\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Network\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;dvportgroup-47\u0026#34; } ], \u0026#34;datastores\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-35\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Datastore\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;datastore-63\u0026#34; } ], \u0026#34;hosts\u0026#34;: [ { \u0026#34;kind\u0026#34;: \u0026#34;Host\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;host-44\u0026#34; }, { \u0026#34;kind\u0026#34;: \u0026#34;Host\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;host-29\u0026#34; } ], \u0026#34;dasEnabled\u0026#34;: false, \u0026#34;dasVms\u0026#34;: [], \u0026#34;drsEnabled\u0026#34;: true, \u0026#34;drsBehavior\u0026#34;: \u0026#34;fullyAutomated\u0026#34;, \u0026#34;drsVms\u0026#34;: [], \u0026#34;datacenter\u0026#34;: null } } } } Creating a validation rule Follow the steps below to create a validation rule by applying a config map custom resource (CR) containing the rule to the Validation service.\nIf the rule has the same name as an existing rule, the Validation service performs an OR operation with the rules.\nIf the rule contradicts a default rule, the Validation service will not start.\nValidation rule example Validation rules are based on virtual machine (VM) attributes collected by the Provider Inventory service.\nFor example, the VMware API uses this path to check whether a VMware VM has NUMA node affinity configured: MOR:VirtualMachine.config.extraConfig[\u0026ldquo;numa.nodeAffinity\u0026rdquo;].\nThe Provider Inventory service simplifies this configuration and returns a testable attribute with a list value:\n\u0026#34;numaNodeAffinity\u0026#34;: [ \u0026#34;0\u0026#34;, \u0026#34;1\u0026#34; ], Create a Rego query, based on this attribute, and add it to the forklift-validation-config config map:\n`count(input.numaNodeAffinity) != 0` Procedure\nCreate a config map CR according to the following example: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: \u0026lt;forklift-validation-config\u0026gt; namespace: konveyor-forklift data: vmware_multiple_disks.rego: |- package \u0026lt;provider_package\u0026gt; (1) has_multiple_disks { (2) count(input.disks) \u0026gt; 1 } concerns[flag] { has_multiple_disks (3) flag := { \u0026#34;category\u0026#34;: \u0026#34;\u0026lt;Information\u0026gt;\u0026#34;, (4) \u0026#34;label\u0026#34;: \u0026#34;Multiple disks detected\u0026#34;, \u0026#34;assessment\u0026#34;: \u0026#34;Multiple disks detected on this VM.\u0026#34; } } EOF The explanations below refer to the callouts in the sample code above.\n(1) Specify the provider package name. Allowed values are io.konveyor.forklift.vmware for VMware and io.konveyor.forklift.ovirt for oVirt. (2) Specify the concerns name and Rego query. (3) Specify the concerns name and flag parameter values. (4) Allowed values are Critical, Warning, and Information. Stop the Validation pod by scaling the forklift-controller deployment to 0: $ kubectl scale -n konveyor-forklift --replicas=0 deployment/forklift-controller Start the Validation pod by scaling the forklift-controller deployment to 1: $ kubectl scale -n konveyor-forklift --replicas=1 deployment/forklift-controller Check the Validation pod log to verify that the pod started: $ kubectl logs -f \u0026lt;validation_pod\u0026gt; If the custom rule conflicts with a default rule, the Validation pod will not start.\nRemove the source provider: $ kubectl delete provider \u0026lt;provider\u0026gt; -n konveyor-forklift Add the source provider to apply the new rule: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: Provider metadata: name: \u0026lt;provider\u0026gt; namespace: konveyor-forklift spec: type: \u0026lt;provider_type\u0026gt; (1) url: \u0026lt;api_end_point\u0026gt; (2) secret: name: \u0026lt;secret\u0026gt; (3) namespace: konveyor-forklift EOF The explanations below refer to the callouts in the sample code above.\n(1) Allowed values are ovirt and vsphere. (2) Specify the API end point URL, for example, https://\u0026lt;vCenter_host\u0026gt;/sdk for vSphere or https://\u0026lt;{rhv-short}_engine_host\u0026gt;/ovirt-engine/api/ for oVirt. (3) Specify the name of the provider Secret CR. Important: Update the rules version after creating a custom rule so that the Inventory service detects the changes and validates the VMs.\nUpdating the inventory rules version Follow the steps below to update the inventory rules version each time the rules are updated so that the Provider Inventory service detects the changes and triggers the Validation service.\nThe rules version is recorded in a rules_version.rego file for each provider.\nProcedure\nRetrieve the current rules version: $ GET https://forklift-validation/v1/data/io/konveyor/forklift/\u0026lt;provider\u0026gt;/rules_version (1) Example output:\n{ \u0026#34;result\u0026#34;: { \u0026#34;rules_version\u0026#34;: 5 } } Connect to the terminal of the Validation pod: $ kubectl rsh \u0026lt;validation_pod\u0026gt; Update the rules version in the /usr/share/opa/policies/io/konveyor/forklift//rules_version.rego file.\nLog out of the Validation pod terminal.\nVerify the updated rules version:\n$ GET https://forklift-validation/v1/data/io/konveyor/forklift/\u0026lt;provider\u0026gt;/rules_version (1) Example output { \u0026#34;result\u0026#34;: { \u0026#34;rules_version\u0026#34;: 6 } } Source\n"},{"uri":"http://konveyor.github.io/forklift/migratingvms/","title":"Migrating virtual machines","tags":[],"description":"","content":"Use this section to configure, perform, and upgrade VM migration and cancel it.\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/","title":"Installing Forklift","tags":[],"description":"","content":"Use this section to set up the environment, install Forklift, upgrade, and uninstall.\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/upgrade/","title":"Upgrading Forklift","tags":[],"description":"","content":"Follow the steps below to upgrade the Forklift Operator using the OKD web console.\nImportant: Do not skip a release when upgrading. For example, upgrade 2.0 to 2.1 and then 2.1 to 2.2.\nIf upgrading from 2.2 to 2.3, see the ### Forklift 2.2 to 2.3 Upgrade Notes section after the procedure\nProcedure\nOpen the OKD web console. Click Operators, Installed Operators, Migration Tookit for Virtualization Operator, then Subscription. Change the update channel to the correct release. See Changing update channel for an Operator in the OKD documentation.\nConfirm that the Upgrade status changes from Up to date to Upgrade available. If it does not, restart the CatalogSource pod:\na. Note the catalog source, for example, redhat-operators. b. Open the command line. c. Retrieve the catalog source pod:\n$ kubectl get pod -n openshift-marketplace | grep \u0026lt;catalog_source\u0026gt; d. Delete the pod: $ kubectl delete pod -n openshift-marketplace \u0026lt;catalog_source_pod\u0026gt; The Upgrade status changes from Up to date to Upgrade available.\nNote: Update approval settings on the Subscriptions tab:\nAutomatic: Starts upgrades automatically. Manual Forces approval to start the upgrade. See Manually approving a pending Operator upgrade in the OKD documentation. Verify that the forklift-ui pod is in a Ready state before logging into the web console: $ kubectl get pods -n konveyor-forklift Example output:\nNAME READY STATUS RESTARTS AGE forklift-controller-788bdb4c69-mw268 2/2 Running 0 2m forklift-operator-6bf45b8d8-qps9v 1/1 Running 0 5m forklift-ui-7cdf96d8f6-xnw5n 1/1 Running 0 2m Forklift 2.2 to 2.3 Upgrade Notes VMware source providers in Forklift If VMware source providers were added to 2.2, upgrading to 2.3 changes the state of any VMware providers to Critical.\nFix: Edit the VMware provider by adding a VDDK init image and verifying the certificate of the VMware provider. For more information see Addding a VMSphere source provider.\nNFS Mapping If the configuration is mapped to NFS on the OKD destination provider in Forklift 2.2, upgrading to Forklift 2.3 invalidates the NFS mapping.\nFix: Edit the AccessModes and VolumeMode parameters in the NFS storage profile. For more information, see Customizing the storage profile.\nSource\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/uninstall/","title":"Uninstalling Forklift","tags":[],"description":"","content":"Uninstall Forklift using the OKD web console or the command line interface (CLI).\nUninstalling Forklift with the OKD web console Follow the steps below to uninstall Forklift using the OKD web console to delete the konveyor-forklift project and custom resource definitions (CRDs).\nPrerequisites\nLog in as a user with cluster-admin privileges. Procedure\nClick Home and then Projects. Locate the konveyor-forklift project. Select Delete Project from the Options menu kebab on the right side of the project. Navigate to the Delete Project pane, enter the project name, and then click Delete. Click Administration and then CustomResourceDefinitions. Type forklift in the Search field to locate the CRDs in the forklift.konveyor.io group. Select Delete CustomResourceDefinition from the Options menu kebab in the right side of each CRD. Uninstalling Forklift with the command line interface Follow the steps below to uninstall Forklift using the command line interface (CLI) by deleting the konveyor-forklift project and the forklift.konveyor.io custom resource definitions (CRDs).\nPrerequisites\nLog in as a user with cluster-admin privileges. Procedure\nDelete the project: $ kubectl delete project konveyor-forklift Delete the CRDs: $ kubectl get crd -o name | grep \u0026#39;forklift\u0026#39; | xargs kubectl delete Delete the OAuthClient: $ kubectl delete oauthclient/forklift-ui Source\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/installation/","title":"Installing the Forklift Operator","tags":[],"description":"","content":"The Forklift Operator can be installed using the OKD web console or the command line interface (CLI).\nInstalling the Forklift Operator using the OKD web console Follow the steps below to install the Forklift Operator by using the OKD web console.\nPrerequisites\nOKD 4.10 installed. KubeVirt Operator installed. Procedure\nLog in with cluster-admin permissions. OPen the OKD web console, click Operators, then OperatorHub. Use the Filter by keyword field to search for forklift-operator. Note: The Forklift Operator is a Community Operator. Red Hat does not support Community Operators.\nClick Migration Tookit for Virtualization Operator and then click **Install. Click Install on the Install Operator page. Click Operators then Installed Operators to verify that Migration Tookit for Virtualization Operator appears in the konveyor-forklift project with the status Succeeded. Click Migration Tookit for Virtualization Operator. Locate the ForkliftController, and click Create Instance under Provided APIs. Click Create. Click Workloads, then Pods to verify that the Forklift pods are running. Log in to the OKD web console. Click Networking then Routes. Select the konveyor-forklift project in the Project: list. The URL for the forklift-ui service that opens the login page for the Forklift web console is displayed.\nClick the URL to navigate to the Forklift web console. Installing the Forklift Operator from the command line interface Follow the steps below to install the Forklift Operator from the command line interface (CLI).\nPrerequisites\nOKD 4.10 installed. KubeVirt Operator installed. Procedure\nLog in with cluster-admin permissions. Create the konveyor-forklift project: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: project.openshift.io/v1 kind: Project metadata: name: konveyor-forklift EOF Create an OperatorGroup CR called migration. $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: migration namespace: konveyor-forklift spec: targetNamespaces: - konveyor-forklift EOF Create a Subscription CR for the Operator: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: forklift-operator namespace: konveyor-forklift spec: channel: development installPlanApproval: Automatic name: forklift-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: \u0026#34;konveyor-forklift-operator.2.3.0\u0026#34; EOF Create a ForkliftController CR: $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: forklift.konveyor.io/v1beta1 kind: ForkliftController metadata: name: forklift-controller namespace: konveyor-forklift spec: olm_managed: true EOF Verify that the Forklift pods are running: $ kubectl get pods -n konveyor-forklift Example output:\nNAME READY STATUS RESTARTS AGE forklift-controller-788bdb4c69-mw268 2/2 Running 0 2m forklift-operator-6bf45b8d8-qps9v 1/1 Running 0 5m forklift-ui-7cdf96d8f6-xnw5n 1/1 Running 0 2m Get the Forklift web console URL with the following command: $ kubectl get route virt -n konveyor-forklift \\ -o custom-columns=:.spec.host The URL for the forklift-ui service that opens the login page for the Forklift web console is displayed.\nExample output:\nhttps://virt-konveyor-forklift.apps.cluster.openshift.com. Getting the Forklift web console URL Using OKD Follow the steps below to retrieve the Forklift web console URL at any time using the OKD web console.\nPrerequisites\nKubeVirt Operator installed. Forklift Operator installed. Procedure\nLog in with cluster-admin privileges. Log in to the OKD web console. Click Networking then Routes. Select the konveyor-forklift project in the Project: list. The URL for the forklift-ui service that opens the login page for the Forklift web console is displayed.\nClick the URL to navigate to the Forklift web console. Getting the Forklift web console URL Using CLI Follow the steps below to retrieve the Forklift web console URL at any time using the command line.\nEnter the following command to get the Forklift web console URL: $ kubectl get route virt -n konveyor-forklift \\ -o custom-columns=:.spec.host The URL for the forklift-ui service that opens the login page for the Forklift web console is displayed.\nExample output:\nhttps://virt-konveyor-forklift.apps.cluster.openshift.com. Launch a browser and navigate to the Forklift web console. Source\n"},{"uri":"http://konveyor.github.io/forklift/installingforklift/prereqs/","title":"Prerequisites","tags":[],"description":"","content":"The following prerequisites ensure the environment is prepared for migration.\nSoftware compatibility guidelines Install compatible software versions with Forklift using the table below.\nForklift OKD KubeVirt VMware vSphere oVirt 2.3 4.10 4.10 6.5 or later 4.4.9 or later Storage support and default modes Forklift uses the following default volume and access modes for supported storage.\nNote: The following settings must be applied if the KubeVirt storage does not support dynamic provisioning:\nFilesystem volume mode: Slower than Block volume mode. ReadWriteOnce access mode: Does not support live virtual machine migration. See Enabling a statically-provisioned storage class for details on editing the storage profile.\nDefault volume and access modes.\nProvisioner Volume mode Access mode kubernetes.io/aws-ebs Block ReadWriteOnce kubernetes.io/azure-disk Block ReadWriteOnce kubernetes.io/azure-file Filesystem ReadWriteMany kubernetes.io/cinder Block ReadWriteOnce kubernetes.io/gce-pd Block ReadWriteOnce kubernetes.io/hostpath-provisioner Filesystem ReadWriteOnce manila.csi.openstack.org Filesystem ReadWriteMany openshift-storage.cephfs.csi.ceph.com Filesystem ReadWriteMany openshift-storage.rbd.csi.ceph.com Block ReadWriteOnce kubernetes.io/rbd Block ReadWriteOnce kubernetes.io/vsphere-volume Block ReadWriteOnce Network prerequisites The following prerequisites apply to all migrations:\nIP addresses, VLANs, and other network configuration settings must not be changed before or after migration. The MAC addresses of the virtual machines are preserved during migration. The network connections between the source environment, the KubeVirt cluster, and the replication repository must be reliable and uninterrupted. A network attachment definition for each additional destination network must be created if mapping more than one source and destination network. Ports Use the following port parameters for migration.\nVMware VSphere The firewalls must enable traffic over the following required network ports for migrating from VMware vSphere.\nPort Protocol Source Destination Purpose 443 TCP OpenShift nodes VMware vCenter VMware provider inventory \u0026amp; Disk transfer authentication 443 TCP OpenShift nodes VMware ESXi hosts Disk transfer authentication 902 TCP OpenShift nodes VMware ESXi hosts Disk transfer data copy oVirt The firewalls must enable traffic over the following required network ports for migrating from oVirt.\nPort Protocol Source Destination Purpose 443 TCP OpenShift nodes oVirt Engine oVirt provider inventory 443 TCP OpenShift nodes oVirt hosts Disk transfer authentication 54322 TCP OpenShift nodes oVirt hosts Disk transfer data copy Source virtual machine prerequisites The following prerequisites apply to all migrations:\nISO/CDROM disks must be unmounted. Each NIC must contain one IPv4 and/or one IPv6 address. The VM name must contain only lowercase letters (a-z), numbers (0-9), or hyphens (-), up to a maximum of 253 characters. The first and last characters must be alphanumeric. The name must not contain uppercase letters, spaces, periods (.), or special characters. The VM name must not duplicate the name of a VM in the KubeVirt environment. The VM operating system must be certified and supported for use as a guest operating system with KubeVirt and for conversion to KVM with virt-v2v. oVirt prerequisites The following prerequisites apply to oVirt migrations:\nMust have the CA certificate of the engine. Able to obtain the CA certificate by navigating to: https://\u0026lt;{rhv-short}_engine_host\u0026gt;/ovirt-engine/services/pki-resource?resource=ca-certificate\u0026amp;format=X509-PEM-CA VMware prerequisites The following prerequisites apply to VMware migrations:\nInstall VMware Tools must be installon all source virtual machines (VMs). If running a warm migration, changed block tracking (CBT) on the VMs and on the VM disks must be enabled. Create a VMware Virtual Disk Development Kit (VDDK) image. Obtain the SHA-1 fingerprint of the vCenter host. The NFC service memory of the host must be increased if migrating more than 10 VMs from an ESXi host in the same migration plan. Creating a VDDK image Forklift uses the VMware Virtual Disk Development Kit (VDDK) SDK to transfer virtual disks from VMware vSphere. Follow the steps below to create the VDDK image.\nNote: The VDDK init image path is required to add a VMware source provider:\nDownload the VMware Virtual Disk Development Kit (VDDK) uild a VDDK image. Push the VDDK image to the image registry. Important: Storing the VDDK image in a public registry might violate the VMware license terms.\nPrerequisites\nOKD image registry. podman installed. KubeVirt must be able to access an external registry if used. Procedure\nCreate and navigate to a temporary directory: Example output: $ mkdir /tmp/\u0026lt;dir_name\u0026gt; \u0026amp;\u0026amp; cd /tmp/\u0026lt;dir_name\u0026gt; Open a browser and navigate to the VMware VDDK download page. Select the latest VDDK version and click Download. Save the VDDK archive file in the temporary directory. Extract the VDDK archive: $ tar -xzf VMware-vix-disklib-\u0026lt;version\u0026gt;.x86_64.tar.gz Create a Dockerfile: $ cat \u0026gt; Dockerfile \u0026lt;\u0026lt;EOF FROM registry.access.redhat.com/ubi8/ubi-minimal COPY vmware-vix-disklib-distrib /vmware-vix-disklib-distrib RUN mkdir -p /opt ENTRYPOINT [\u0026#34;cp\u0026#34;, \u0026#34;-r\u0026#34;, \u0026#34;/vmware-vix-disklib-distrib\u0026#34;, \u0026#34;/opt\u0026#34;] EOF Build the VDDK image: $ podman build . -t \u0026lt;registry_route_or_server_path\u0026gt;/vddk:\u0026lt;tag\u0026gt; Push the VDDK image to the registry: $ podman push \u0026lt;registry_route_or_server_path\u0026gt;/vddk:\u0026lt;tag\u0026gt; Ensure that the image is accessible to the KubeVirt environment. Obtaining the SHA-1 fingerprint of a vCenter host Follow the steps below to obtain the SHA-1 fingerprint of a vCenter host in order to create a Secret CR.\nProcedure\nRun the following command: $ openssl s_client \\ -connect \u0026lt;vcenter_host\u0026gt;:443 \\ (1) \u0026lt; /dev/null 2\u0026gt;/dev/null \\ | openssl x509 -fingerprint -noout -in /dev/stdin \\ | cut -d \u0026#39;=\u0026#39; -f 2 1\tSpecify the IP address or FQDN of the vCenter host. Example output:\n01:23:45:67:89:AB:CD:EF:01:23:45:67:89:AB:CD:EF:01:23:45:67 Increasing the NFC service memory of an ESXi host Note: If more than 10 VMs are migrating from an ESXi host in the same migration plan, increase the NFC service memory of the host.\nThe migration will fail because the NFC service memory is limited to 10 parallel connections.\nProcedure\nLog in to the ESXi host as root. Change the value of maxMemory to 1000000000 in /etc/vmware/hostd/config.xml: ... \u0026lt;nfcsvc\u0026gt; \u0026lt;path\u0026gt;libnfcsvc.so\u0026lt;/path\u0026gt; \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt; \u0026lt;maxMemory\u0026gt;1000000000\u0026lt;/maxMemory\u0026gt; \u0026lt;maxStreamMemory\u0026gt;10485760\u0026lt;/maxStreamMemory\u0026gt; \u0026lt;/nfcsvc\u0026gt; ... Restart hostd: # /etc/init.d/hostd restart The host does not need to be restarted.\nSource\n"},{"uri":"http://konveyor.github.io/tackle/tackle1/upgrade/","title":"Upgrading Tackle","tags":[],"description":"","content":"Tackle application instances are upgraded manually.\nUpgrading from version 1.1.0 to 1.2.0 Follow the steps below to manually upgrade an instance of the Tackle application from 1.1.0 to 1.2.0.\nPrerequisites\nProject administrator privileges. Procedure\nImportant: Specify the namespace and the Tackle instance name for each step.\nUpdate the Keycloak deployment of the Tackle instance: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-keycloak keycloak-theme=quay.io/konveyor/tackle-keycloak-init:1.2.0 Update the application-inventory-rest deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-application-inventory-rest \\ \u0026lt;tackle_instance\u0026gt;-application-inventory-rest=quay.io/konveyor/tackle-application-inventory:1.2.0-native Update the controls-rest deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-controls-rest \\ \u0026lt;tackle_instance\u0026gt;-controls-rest=quay.io/konveyor/tackle-controls:1.2.0-native Update the pathfinder-rest deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-pathfinder-rest \\ \u0026lt;tackle_instance\u0026gt;-pathfinder-rest=quay.io/konveyor/tackle-pathfinder:1.2.0-native Update the UI deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-ui \\ \u0026lt;tackle_instance\u0026gt;-ui=quay.io/konveyor/tackle-ui:1.2.0 Log in to the web console and click the Help icon beside the user name to verify the upgrade. The About Tackle window opens and displays the version number.\nUpgrading from version 1.0.0 to 1.1.0 Follow the steps below to manually upgrade an instance of the Tackle application from 1.0.0 to 1.1.0.\nPrerequisites\nProject administrator privileges. Procedure\nImportant: Specify the namespace and the Tackle instance name for each step.\nUpdate the Keycloak deployment of the Tackle instance: $ kubectl -n \u0026lt;namespace\u0026gt; exec deployments/\u0026lt;tackle_instance\u0026gt;-keycloak \\ -c \u0026lt;tackle_instance\u0026gt;-keycloak -- bash -c \u0026#39;/opt/jboss/keycloak/bin/kcadm.sh \\ update realms/tackle -s internationalizationEnabled=true -s supportedLocales+=en \\ -s supportedLocales+=es -s defaultLocale=en --server http://localhost:8080/auth \\ --realm master --user $KEYCLOAK_USER --password $KEYCLOAK_PASSWORD\u0026#39; Update the application-inventory-rest deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-application-inventory-rest \\ \u0026lt;tackle_instance\u0026gt;-application-inventory-rest=quay.io/konveyor/tackle-application-inventory:1.1.0-native Update the controls-rest deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-controls-rest \\ \u0026lt;tackle_instance\u0026gt;-controls-rest=quay.io/konveyor/tackle-controls:1.1.0-native Update the pathfinder-rest deployment: $ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-pathfinder-rest \\ \u0026lt;tackle_instance\u0026gt;-pathfinder-rest=quay.io/konveyor/tackle-pathfinder:1.1.0-native 4.Update the UI deployment:\n$ kubectl set image -n \u0026lt;namespace\u0026gt; deployment/\u0026lt;tackle_instance\u0026gt;-ui \\ \u0026lt;tackle_instance\u0026gt;-ui=quay.io/konveyor/tackle-ui:1.1.0 Log in to the web console and click the Help icon beside the user name to verify the upgrade. The About Tackle window opens and displays the version number.\nSource\n"},{"uri":"http://konveyor.github.io/tackle/tackle1/webconsolesvcs/","title":"Web Console Services","tags":[],"description":"","content":"Tackle web console provides the following services:\nApplication inventory Assessments Reviews Reports Controls Application inventory The Application inventory page can perform the following tasks:\nManage application portfolios. Define and manage application dependencies. Link applications to the business services that they support. Describe and categorize applications by using tags. Assessments The applications are assessed on the Application inventory page which helps determine the appropriate migration strategy for each application:\nRehosting Replatforming Refactoring Reviews Review a completed assessment on the Application inventory page where the following information is collected from each reviewer:\nProposed action: Best alternative for modernizing and migrating the application. Effort estimate: The likely amount of effort involved in migrating the application based on the proposed action and the identified risks. Business criticality: Estimate of how critical the application is to the business on a scale of 1 to 10. Work priority: Estimated priority for the proposed action on a scale of 1 to 10. The Reports page displays the results of the reviews.\nReports The Reports page displays the results of the application assessments and reviews and can help plan the migration by providing the following information:\nSuitability of applications for containerization. Possible risks and severity. Adoption plan based on estimated effort, work priority, and application dependencies. Controls The Controls page can manage the following entities:\nStakeholder: Individual with an interest in an application or a subject matter expert. Stakeholder group: Related stakeholders who may belong to one or more stakeholder groups used to assign multiple stakeholders to review an assessment. Job function: Attribute of a stakeholder using a predefined job function or by creating a new job function. Business service: Attribute of an application, for example, credit card service, transportation, or IT support using a predefined business service or by creating a new business service. Tag: Attribute of an application that are an extensible and flexible way to add metadata to applications. Tags are grouped under a parent tag type using a predefined tag or by creating a new tag. Tag type: Defines the rank and background color of a group of related tags. Source\n"},{"uri":"http://konveyor.github.io/tackle/tackle1/manageusers/","title":"Managing users and credentials","tags":[],"description":"","content":"Follow the procedures in this section to manage Tackle users and passwords using the Keycloak admin console.\nAccessing the Keycloak admin console A Keycloak admin user is created when Tackle is installed. The admin username and password are needed to log in to the Keycloak admin console. The credentials are stored in the tackle-keycloak secret.\nPrerequisites\nCluster-admin privileges. Procedure\nRun the following command to obtain the admin credentials: $ kubectl get secret tackle-keycloak -o go-template=\u0026#39;{{range $k,$v := .data}}{{printf \u0026#34;%s: \u0026#34; $k}}{{if not $v}}{{$v}}{{else}}{{$v | base64decode}}{{end}}{{\u0026#34;\\n\u0026#34;}}{{end}}\u0026#39; Example output ADMIN_PASSWORD: \u0026lt;password\u0026gt; ADMIN_USERNAME: admin Launch a browser and navigate to https://\u0026lt;www.example.com\u0026gt;/auth and specify the Tackle cluster URL. Log in to the Keycloak admin console with the admin user name and password. Changing the default password Follow the steps below to change the default password of the tackle user.\nPrerequisites\nCluster-admin privileges Keycloak admin user name and password. Procedure\nLog in to the Keycloak admin console at https://\u0026lt;www.example.com\u0026gt;/auth and specify the Tackle cluster URL. Locate the tackle user in the Tackle realm. See Searching for users in the Keycloak documentation. Update the tackle user’s password. See User Credentials. Log out of the Keycloak admin console. Log in to the Tackle web console as the tackle user to verify the new password. Adding users Follow the steps below to add users by using the Keycloak admin console.\nPrerequisites\nCluster-admin privileges Keycloak admin user name and password. Procedure\nLog in to the Keycloak admin console at https://\u0026lt;www.example.com\u0026gt;/auth and specify the Tackle cluster URL. Create a new user in the Tackle realm. See Creating a new user in the Keycloak documentation. Create a password for the new user. See Creating a password for the user. Optional: Set attributes and permissions for the new user. See User profile.\nAdditional resources for Keycloak Admin console in the Keycloak documentation. User management in the Keycloak documentation. Keycloak Operator on Kubernetes in the Keycloak documentation. Source\n"},{"uri":"http://konveyor.github.io/tackle/tackle1/manageassess/","title":"Managing assessments","tags":[],"description":"","content":"Start, edit, review, and delete application assessments in the Application inventory page of the Tackle web console.\nStarting an assessment Follow the steps below to start an application assessment on the Application inventory page of the Tackle web console.\nProcedure\nStart the Tackle web console Click Application Inventory. Select an application that does not have a Completed assessment status and click the Assess button in the toolbar. Select individual stakeholders or stakeholder groups and then click Next. Select responses to all questions on each page of the Application Assessment wizard and then click Next. Important: All questions are mandatory.\nClick Save to save the assessment, or Save and Review to start the assessment review process. The Assessment status of each assessed application is set to Completed.\nEditing an assessment Follow the steps below to take an application assessment on the Application inventory page of the Tackle web console.\nPrerequisites\nAn application must have a Completed assessment status. Procedure\nOpen the Tackle web console. Click Application Inventory. Select an application that does not have a Completed assessment status and click the Assess button in the toolbar. Click Continue to confirm editing the assessment. Update entries. Click Save to save the assessment, or Save and Review to start the assessment review process. ##Reviewing an assessment Follow the steps below to review an application assessment on the Application inventory page of the Tackle web console.\nPrerequisites\nAn application must have a Completed assessment status and a Not started review status. Procedure\nOpen the Tackle web console. Click Application Inventory. Select an application with a Completed assessment status and a Not Started review status and click the Review button in the toolbar. Review the assessment in the Assessment summary section on the Review page. Note: The Assessment summary table contains a Risk column that indicates the severity of the risk associated with each response.\nSelect a Proposed action and an Effort estimate. Set values for Business criticality and Work priority. Recommended: Enter comments in the Comments field.\nClick Submit Review. The Review status of each application is set to Completed.\nExpand the application to view the review results. Copying and applying assessments and reviews. Follow the steps below to:\nCopy an assessment or assessment and review from a single application and apply them to multiple applications on the Application inventory page of the Tackle web console. Apply assessments and reviews to groups of related applications, for example, applications written in Java or belonging to the same business service. Prerequisites\nAn application must have a Completed assessment status or Completed assessment and review statuses, depending on whether an assessment or an assessment and review is being copied. Procedure\nOpen the Tackle web console. Click Application Inventory. Click the Options menu kebab beside an application with a Completed assessment status or Completed assessment and Review statuses. Select Copy assessment or Copy assessment and review. Optional: In the dialog box, click Name to select a filter, for example:\nTag and select a tag. Java to display a filtered list of applications. Select the applications to apply the copied assessment or assessment and review to. If a selected application has an existing assessment or review, select the Yes, continue check box to confirm that existing assessments and reviews will be overwritten. Click Copy. The selected applications are set to a Completed assessment status or Completed assessment and review statuses.\nDeleting an assessment Follow the steps below to delete an application assessment on the Application inventory page of the Tackle web console. Deleting an assessment deletes its review.\nPrerequisites\nAn application must have a Completed assessment status. Procedure\nOpen the Tackle web console. Click Application Inventory. Click he Options menu kebab beside an application with a Completed assessment status and select Discard assessment. Click Continue to confirm the deletion. The Assessment and Review status of the application are set to Not started.\nSource\n"},{"uri":"http://konveyor.github.io/tackle/tackle1/manageapps/","title":"Managing applications","tags":[],"description":"","content":" Managing Applications Follow the procedures in this section to create, import, tag, and modify the applications in the Application inventory page of the Tackle web console.\nCreating an application Follow the steps below to create an application on the Application inventory page of the Tackle web console.\nProcedure\nOpen the Tackle web console. Click Application inventory and then Create New. Complete the following fields: Name: Name of the application. Description: Optional. Description of the application. Business service: Optional. Select a business service that describes the application. Tags: Optional. Select one or more tags. Comments: Optional. Comments about the application. Click Create. The new application is displayed on the Application inventory page. Expand the application to view its tags and comments.\nImporting applications Follow the steps below to import one or more applications into the Application inventory page of the Tackle web console by using a CSV file.\nNote: Do not create tags or business services by importing a CSV file. Specified tags or business services must exist in the web console before importing the applications.\nThe CSV file contains the following fields:\nRecord Type 1: Describes an application or application dependencies. Required for all records. The following values are allowed for Record Type 1:\nApplication: This option has the following fields: Application Name: Required Description: Optional Comments: Optional Business Service: Optional. Must exist in the web console Tag Type \u0026lt;1..20\u0026gt;: Optional. Must exist in the web console. Tag \u0026lt;1..20\u0026gt;: Optional. Must exist in the web console. Note: Import up to 20 Tag Type and Tag fields.\nApplication dependencies: This option requires the following fields: Application Name Dependency: Must be the same as the Application Name of the dependency. Dependency Direction: Allowed values are northbound and southbound. All other fields are empty. See the CSV example below.\nRecord Type 1 Application Name Dependency Dependency Direction Description Comments Business Service Tag Type 1 Tag 1 Tag Type 2 Tag 2 Tag Type 3 Tag 3 1 Imported-Purchasing Vendor management Required for purchase order processing and accounts payable Finance and HR Operating System Z/OS Database DB2 Language COBOL 2 Imported-Purchasing Tiller northbound 1 Imported-PO Requisitions, purchase orders, goods received Requisition to receipt Finance and HR Operating System Z/OS Database DB2 Language COBOL 2 Imported-Payroll Imported-GL northbound 2 Imported-Payroll Imported-HR southbound 1 Imported-GL General Ledger Finance and HR Operating System Z/OS Database DB2 Language COBOL 1 Imported-HR Human Resources Go live scheduled for Q3 Finance and HR Operating System RHEL 8 Database PostgreSQL Language Python Prerequisites\nValid CSV file. Specified business services, tag types, and tags created in the web console.\nProcedure\nOpen the Tackle web console. Click Application Inventory. Click the Options menu kebab in the toolbar and select Import. Browse to the CSV file and click Open. Click Import. The imported applications are displayed on the Application inventory page.\nManaging application imports Follow the steps below to manage application imports on the Application import page of the Tackle web console.\nProcedure\nOpen the Tackle web console. Click Application Inventory. Click the Options menu kebab in the toolbar and select Manage Imports. The Application import page displays a list of application imports.\nClick the Options menu kebab beside an application import and select one of the following options: Delete: Deletes the application import. View Error: Report displays a table of application import errors. Export Errors: Saves the application import errors as a CSV file. Updating the tags assigned to an application Follow the steps below to add or remove tags assigned to an application on the Application inventory page of the Tackle web console.\nProcedure\nOpen the Tackle web console. Click Application Inventory. Click the Edit icon beside an application. Add or delete tags and click Save. Expand the application to view the updated tags. Updating the business service assigned to an application Follow the steps below to update the business service assigned to an application on the Application inventory page of the Tackle web console.\nPrerequisites\nThe business service must exist on the Controls page. Procedure\nOpen the Tackle web console. click Application Inventory. Click the Edit icon beside an application. Select a business service and click Save. The updated business service is displayed in the Business service column of the application.\nManaging application dependencies Follow the steps below to add, delete, and view application dependencies in the Manage dependencies window on the Application inventory page of the Tackle web console.\nProcedure\nOpen the Tackle web console. Click Application Inventory. Click the Options menu kebab beside an application and select Manage Dependencies. Do one of the following: To add dependencies: Select applications in the northbound or southbound dependencies fields. To remove dependencies: Delete selected applications in the northbound or southbound dependencies fields. Click Close to save and close. Source\n"},{"uri":"http://konveyor.github.io/tackle/tackle1/additionaltools/","title":"Additional tools","tags":[],"description":"","content":"DIVA Tackle DiVA is a data-centric application analysis tool that imports a set of target application source files and provides database/transaction analysis results.\nTest Generator Tackle Test Generator is an automatic test-generation and differential-testing tool that currently supports unit-level test generation for Java applications. (Future project plans include adding capabilities for automated generation of end-to-end UI/UX test cases for web applications and test cases for REST APIs.)\nContainer Advisor Tackle Container Advisor provides containerization advisory information for a large scale application portfolio. It takes a natural language description of applications and recommends whether the applications can be containerized in terms of images from multiple container catalogs (DockerHub, Openshift). (Future project plans include supporting Operators and recommending disposition that is explainable.)\nSource\n"},{"uri":"http://konveyor.github.io/tackle/installation1/","title":"Installing Tackle 1.0","tags":[],"description":"","content":"Follow the steps below to provision the minikube cluster and install Tackle 1.2.\nProvisioning Minikube Follow the steps below to provision minikube for single users deploying Tackle on a workstation. These steps will configure minikube and enable:\nIngress so the Tackle tool can publish outside of the Kubernetes cluster. Operator lifecycle manager (OLM) addon. (OpenShift has OLM installed out of the box but Kubernetes does not.) Procedure\nProvision the minikube cluster with these recommended parameters. Replace \u0026lt;profile name\u0026gt; with your choice of minikube profile name. [user@user ~]$ minikube start --memory=10g -p \u0026lt;profile name\u0026gt; Enable the ingress addon. [user@user ~]$ minikube addons enable ingress -p \u0026lt;profile name\u0026gt; Install Operator Lifecycle Manager (OLM), a tool to help manage the Operators running on your cluster. [user@user ~]$ curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.21.2/install.sh | bash -s v0.21.2 Installing the Tackle Operator Follow the steps below to download and install the Tackle Operator on an Enterprise Kubernetes Platform cluster.\nPrerequisites\nCluster-admin privileges. This can be minikube. Procedure\nInstall the Tackle Operator: [user@user ~]$ kubectl create -f https://operatorhub.io/install/tackle-operator.yaml The Tackle Operator is installed in the my-tackle-operator namespace by default.\nVerify the Tackle Operator installation by viewing its resources: [user@user ~]$ kubectl get all -n my-tackle-operator Installing the Tackle application Follow the steps below to install Tackle in a namespace by creating an instance of the Tackle application.\nPrerequisites\nTackle Operator installed on the cluster. Project-admin privileges. Procedure\nCreate an instance of the Tackle application, specifying its namespace: [user@user ~]$ kubectl apply -f https://raw.githubusercontent.com/konveyor/tackle-operator/main/src/main/resources/k8s/tackle/tackle.yaml -n \u0026lt;namespace\u0026gt; Note: Multiple instances of the Tackle application can be created in the same namespace by specifying a unique name for each instance in the tackle.yaml file.\nVerify the instance. [user@user ~]$ kubectl get pods -n \u0026lt;namespace\u0026gt; Repeat this step until all components are Completed or Running. Note: This can take one to five minutes depending on the cluster resources.\nLogging into the Tackle web console Follow the steps below to log into the Tackle web console.\nPrerequisites\nTackle application installed. Procedure\nIf using minikube, access the minikube dashboard. This will enable the dashboard add-on, and open the proxy in the default web browser. Or navigate to your kubernetes cluster dashboard. [user@user ~]$ minikube dashboard -p \u0026lt;profile name\u0026gt; Ensure the top dropdown namespace selector is set to the \u0026lt;namespace\u0026gt; you created the Tackle application in. Click Service then Ingresses Click the endpoint IP for the tackle-sample ingress ingress to launch the Tackle web console in a new browser window. Note: This may default to http://$IP_ADDR and fail to load, if so switch to https://$IP_ADDR\nThe default auth enabled credentials are: tackle/password Important: Change the default password of the tackle user.\nSource\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step1export/","title":"Step One: Export Resources","tags":[],"description":"","content":"The first step of the cluster migration process is exporting resources from a source cluster of any namespace to be input for the subsequent commands.\nAll of the following export commands will output the contents of the foo namespace into a local export directory with the context demo defined in KUBECONFIG.\ncrane export -n foo -e export --context demo cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; conf.yaml namespace: foo export-dir: export context: demo EOF crane export -c conf.yaml cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; conf.yaml namespace: foo export-dir: export context: testing EOF crane export -c conf.yaml --context demo # Note the difference is we are overriding \u0026#34;context\u0026#34; here with flag Note: There are multiple ways to input a command, precedence of which is input from flags \u0026gt; input from config file \u0026gt; env variables \u0026gt; default values (not all the flags can have a corresponding env variable). This behavior persists across all Crane CLI commands.\nSource\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step2transform/","title":"Step Two: Transform Exports","tags":[],"description":"","content":"The Transform command facilitates the changes to the exported resources that are frequently necessary when migrating workloads between one environment to another. For example:\nStripping the resource status information that is no longer relevant after the resource is serialized out of a cluster. Adjusting resource quotas to fit the destination environment. Altering node selectors to match the new environment if the node labels do not match the source environment. Applying custom labels or annotations to resources during the migration. Because most changes are specific to an environment, Crane is designed for total customization and the transform command accepts a “plugins” directory argument.\nEach plugin is an executable with a well defined stdin/out interface allowing for customization or installation and use of published generic plugins.\nAfter exporting the resources into a local directory and installing the desired transformation plugins, the crane transform command can run. The output is placed in a directory with a set of transform files that describe the changes that need to be applied to the original resources before their final import. The changes are written in the JSONPatch format, are human readable, and easily hackable.\nThis command generates a patch to add an annotation transform-test:test for objects in the export directory and the transform directory to be used as input for apply command.\ncrane transform -e export -p plugins -t transform --optional-flags=\u0026#34;add-annotations=transform-test:test\u0026#34; Run the following to see the list of available optional commands for configured plugins.\ncrane transform optionals See Managing PlugIns for more information on plugins that can be consumed by the transform command.\nSource\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/step3apply/","title":"Step Three: Apply Patches","tags":[],"description":"","content":"The final step of the cluster migration process is to apply all the patches generated by the Transform command to exported resources.\ncrane apply -e export -t transform -o output Apply the patches in the transform directory to the resources in the export directory and save the modified resource files in the output directory.\nAfter applying the patches, the resources located in output directory can either be deployed to the destination cluster using kubectl apply, or they can be pushed to a repository and then applied with the help of the GitOps pipeline. An example of the later scenario can be found here.\nSource\n"},{"uri":"http://konveyor.github.io/crane/tutorials/","title":"Tutorials","tags":[],"description":"","content":"Use this section to follow two examples of how to migrate Kubernetes workloads and their state between clusters, remove environment-specific configuration, and automate application deployments along the way.\n"},{"uri":"http://konveyor.github.io/crane/tools/","title":"Tools","tags":[],"description":"","content":"Use this section to better understand the Crane tools.\n"},{"uri":"http://konveyor.github.io/crane/usingcrane/","title":"Using Crane","tags":[],"description":"","content":"Use this section to complete the three steps of the Crane migration process.\nStep 1: Exporting resources Step 2: Transforming exports Step 3: Applying patches "},{"uri":"http://konveyor.github.io/crane/tools/gitopsintegration/","title":"Integrating GitOps","tags":[],"description":"","content":"All Crane commands are individual utilities, but when used together in sequence, they form a pipeline.\nCrane makes it easy to integrate a gitops that applies the patches/resources generated at the end of the apply command on the destination cluster. The resources generated at the end of the process (i.e export, transform, apply) can be pushed to a github repository, and a pipeline can be created to deploy the resources on a cluster on every push.\nSource\n"},{"uri":"http://konveyor.github.io/crane/tools/customplugins/","title":"Developing custom plugins","tags":[],"description":"","content":"This document covers how to write a plugin binary using crane-lib. It requires:\nProcedure\nGo to the development environment setup. (Optionally, an overview of the crane toolkit.)\nCreate binary plugin for crane-lib as a simple Go program in the following format that will:\nRead an input from stdin. Call the Run function with the input object passed as unstructured. Print the return value of Run function on stdout. package main import ( \u0026#34;fmt\u0026#34; jsonpatch \u0026#34;github.com/evanphx/json-patch\u0026#34; \u0026#34;github.com/konveyor/crane-lib/transform\u0026#34; \u0026#34;github.com/konveyor/crane-lib/transform/cli\u0026#34; ) func main() { fields := []transform.OptionalFields{ { FlagName: \u0026#34;my-flag\u0026#34;, Help: \u0026#34;What the flag does\u0026#34;, Example: \u0026#34;true\u0026#34;, }, } cli.RunAndExit(cli.NewCustomPlugin(\u0026#34;MyCustomPlugin\u0026#34;, \u0026#34;v1\u0026#34;, fields, Run)) } func Run(request transform.PluginRequest) (transform.PluginResponse, error) { // plugin writers need to write custom code here. resp := transform.PluginResponse{ Version: string(transform.V1), } // prepare the response return resp, nil } The json is passed in using stdin is a transform.PluginRequest which consists of an inline unstructured object and an optional Extras map containing additional flags. Without any Extras the format is identical to the json output from a kubectl get -o json call.\nWhen adding extra parameters, a map field “extras” is added at the top level (parallel to “apiVersion”, “kind”, etc.).\nVersion the plugin development output by passing in the JSOC object on stdin manually during development. For example, if the code above is compiled and run with the following json as input, the output will be {\u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot;}.\n./my-plugin { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Route\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;openshift.io/host.generated\u0026#34;: \u0026#34;true\u0026#34; }, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34;, \u0026#34;managedFields\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:spec\u0026#34;: { \u0026#34;f:path\u0026#34;: {}, \u0026#34;f:to\u0026#34;: { \u0026#34;f:kind\u0026#34;: {}, \u0026#34;f:name\u0026#34;: {}, \u0026#34;f:weight\u0026#34;: {} }, \u0026#34;f:wildcardPolicy\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;oc\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34; }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;route.openshift.io/v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:status\u0026#34;: { \u0026#34;f:ingress\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;openshift-router\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;mssql-app-route\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;mssql-persistent\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;155816271\u0026#34;, \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/route.openshift.io/v1/namespaces/mssql-persistent/routes/mssql-app-route\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;42dca205-31bf-463d-b516-f84064523c2c\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;mssql-app-route-mssql-persistent.apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;to\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mssql-app-service\u0026#34;, \u0026#34;weight\u0026#34;: 100 }, \u0026#34;wildcardPolicy\u0026#34;: \u0026#34;None\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;ingress\u0026#34;: [ { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2021-06-10T04:11:21Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Admitted\u0026#34; } ], \u0026#34;host\u0026#34;: \u0026#34;mssql-app-route-mssql-persistent.apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;routerCanonicalHostname\u0026#34;: \u0026#34;apps.cluster-alpatel-aux-tools-444.alpatel-aux-tools-444.mg.dog8code.com\u0026#34;, \u0026#34;routerName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;wildcardPolicy\u0026#34;: \u0026#34;None\u0026#34; } ] } } When the plugin is ready to be tested, put it in a directory and run with the crane cli command.\nMore accurate detail can be found [here] (https://github.com/konveyor/crane-lib/blob/main/transform/binary-plugin/README.md).\nSource\n"},{"uri":"http://konveyor.github.io/crane/tools/tunnelapi/","title":"Tunnel API","tags":[],"description":"","content":"The tunnel-api sub-command can be used to access an on-premise cluster from a cloud cluster to allow orchestrating migrations from on-premise clusters using MTC where access is not possible otherwise.\nAn openvpn client on the on-premise cluster will connect to a server running on the cloud cluster and the openvpn server is exposed to the client using a load balancer address on the cloud cluster.\nA service created on the cloud cluster is used to expose the on-premise clusters API to MTC running on the cloud cluster.\nRequirements The system used to create the VPN tunnel must have access and be logged in to both clusters. It must be possible to create a load balancer on the cloud cluster. An available namespace on each cluster to run the tunnel in not created in advance. Note: To connect multiple on-premise source clusters to the cloud cluster use a separate namespace for each.\napi-tunnel options namespace: The namespace used to launch the VPN tunnel in, defaults to openvpn destination-context: The cloud destination cluster context where the openvpn server will be launched. destination-image: The container image to use on the destination cluster. (Default: quay.io/konveyor/openvpn:latest) source-context: The on-premise source cluster context where the openvpn client will be launched. source-image: The container image to use on the source cluster. (Default: quay.io/konveyor/openvpn:latest) proxy-host: The hostname of an http-proxy to use on the source cluster for connecting to the destination cluster. proxy-pass: The password for the http-proxy. If specified, also specify a username or it will be ignored. proxy-port: The port the http-proxy is listening on. If none is specified it will default to 3128 proxy-user: The username for the http-proxy. If specified, also specify a password or it will be ignored. Example\ncrane tunnel-api --namespace openvpn-311 \\ --destination-context openshift-migration/c131-e-us-east-containers-cloud-ibm-com/admin \\ --source-context default/192-168-122-171-nip-io:8443/admin \\ --source-image: my.registry.server:5000/konveyor/openvpn:latest \\ --proxy-host my.proxy.server \\ --proxy-port 3128 \\ --proxy-user foo \\ --proxy-pass bar MTC Configuration When configuring the source cluster in MTC the API URL takes the form of https://proxied-cluster.${namespace}.svc.cluster.local:8443.\nOptional: Set the image registry for direct image migrations to proxied-cluster.${namespace}.svc.cluster.local:5000.\nReplace ${namespace} with either openvpn or the specified namespace when running the command to set up the tunnel.\nDemo https://youtu.be/wrPVcZ4bP1M\nTroubleshooting It may take 3 to 5 minutes after the setup to complete for the load balancer address to become resolvable. During this time the client will be unable to connect and establish a connection and the tunnel will not function.\nDuring this time, run oc get pods in the specified namespace for setup, and monitor the logs of the openvpn container to see the connection establish.\nExample\noc logs -f -n openvpn-311 openvpn-7b66f65d48-79dbs -c openvpn Source\n"},{"uri":"http://konveyor.github.io/crane/tools/pluginmanager/","title":"Plugin Manager","tags":[],"description":"","content":"The Plugin Manager is an optional utility that assists in adding plugins to the appropriate location to be consumed by the transform command.\nList Plugin utility The List Plugin utility discovers available plugins that that are compatible with the current OS and architecture.\ncrane plugin-manager list Listing from the repo default +-----------------+------------------+-------------------+ | NAME | SHORTDESCRIPTION | AVAILABLEVERSIONS | +-----------------+------------------+-------------------+ | OpenshiftPlugin | OpenshiftPlugin | v0.0.1 | +-----------------+------------------+-------------------+ Other valid execution examples This command lists all installed plugins managed by plugin-manager.\ncrane plugin-manager --installed -p plugin-dir This command lists all version of the foo plugin with detailed information.\ncrane plugin-manager --params -n foo Add Plugin utility The Add Plugin utility places the plugin into a directory to be consumed by Transform command.\nThis command downloads the binary of the foo version 0.0.1 plugin from the appropriate source and places it in the plugin-dir/managed directory (the default is plugins).\ncrane plugin-manager add foo --version 0.0.1 -p plugin-dir Remove Plugin utility The Remove Plugin utility removes unwanted plugins from being consumed by the Transform command. This command removes the foo plugin from the plugin-dir/managed dir.\ncrane plugin-manager remove foo -p plugin-dir Note: The plugin-manager command operates in the \u0026lt;plugin-dir\u0026gt;/managed directory. Whenever the flag -p, plugin-dir is used with plugin-manager, the utility operates in the managed places folder in \u0026lt;plugin-dir\u0026gt;.\nFor example: plugin-manager add places the plugin binary within \u0026lt;plugin-dir\u0026gt;/managed, plugin-manager removes the binary from \u0026lt;plugin-dir\u0026gt;/managed, and plugin-manager list --installed uses the path \u0026lt;plugin-dir\u0026gt;/managed to list installed plugins.\nManual plugin management Currently only two plugins are available with more plugins available soon.\nAvailable plugins: -Kubernetes (build into crane-lib) -OpenShift.\nThese plugins can be added to the desired plugin directory. (The default directory is plugin where crane is installed.)\nImportant: The Kubernetes plugin is built into the crane-lib and is not to be added manually or otherwise.\nTo install the plugins:\nDownload the binary of the plugin from the release and place it in the plugin directory.\ncurl -sL https://api.github.com/repos/konveyor/crane-plugin-\u0026lt;plugin-name\u0026gt;/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;\u0026lt;arch\u0026gt;-\u0026lt;os\u0026gt;\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- chmod +x \u0026lt;binary\u0026gt; cp \u0026lt;binary\u0026gt; /bin/usr/crane/plugins/\u0026lt;plugin-name\u0026gt; Build the binary locally and place it in theplugin directory.\ncd $GOPATH git clone https://github.com/konveyor/crane-plugin-\u0026lt;plugin-name\u0026gt;.git cd crane-plugin-\u0026lt;plugin-name\u0026gt; go build -f \u0026lt;plugin-name\u0026gt; . cp \u0026lt;plugin\u0026gt; /bin/usr/crane/plugins/\u0026lt;plugin-name\u0026gt; Note: Adding plugins available in the plugin repo manually is not advisable as long as it can be added usingplugin-manager. For custom plugins or testing plugins under development, manual management is necessary.\nSource\n"},{"uri":"http://konveyor.github.io/crane/tutorials/statelessappmirror/","title":"Stateless application mirror","tags":[],"description":"","content":"This tutorial is an example of how to mirror a simple, stateless PHP Guestbook application using Crane.\nRoadmap\n1. Deploy the Guestbook application in the source cluster. 2. Extract resources from the source cluster using Crane Export. 3. Transform resources to prepare manifests for the destination cluster using Crane Transform. 4. Apply the transformations using Crane Apply. Apply application manifests to the destination cluster. Prerequisites\nCreate a source and destination Kubernetes cluster environment in minikube or Kind: minikube\nwget \u0026#34;https://raw.githubusercontent.com/konveyor/crane/main/hack/minikube-clusters-start.sh\u0026#34; chmod +x minikube-clusters-start.sh./minikube-clusters-start.sh Kind\nwget \u0026#34;https://raw.githubusercontent.com/konveyor/crane-runner/main/hack/kind-up.sh\u0026#34; chmod +x kind-up.sh./kind-up.sh Install Crane using the Installation Guide. Important: Read through Kubernetes\u0026rsquo; documentation on accessing multiple clusters. This document references src and dest contexts that refer to the clusters created using the minikube startup scripts above.\nWhen working in the home environment, or use kind (kind-src and kind-dest), modify the commands below to reference the correct cluster context.\n1. Deploy the Guestbook application in the source cluster Deploy the Kubernetes\u0026rsquo; stateless guestbook application and modify it to be consumable with Kustomize (Kubernetes native and template-free tool to manage application configuration). The guestbook application consists of:\nredis leader deployment and service redis follower deployment and service guestbook front-end deployment and service kubectl --context src create namespace guestbook kubectl --context src --namespace guestbook apply -k github.com/konveyor/crane-runner/examples/resources/guestbook kubectl --context src --namespace guestbook wait --for=condition=ready pod --selector=app=guestbook --timeout=180s Optional\nForward localhost traffic to the frontend of the Guestbook application to access Guestbook from a browser using localhost:8080:\nkubectl --context src --namespace guestbook port-forward svc/frontend 8080:80 2. Extract from the source cluster Crane’s export command extracts all of the specified resources from the “source” cluster.\ncrane export --context src --namespace guestbook Check the export directory to verify it is working correctly. The directory should look similar to the example below:\n$ tree -a export export ├── failures │ └── guestbook └── resources └── guestbook ├── ConfigMap_guestbook_kube-root-ca.crt.yaml ├── Deployment_guestbook_frontend.yaml ├── Deployment_guestbook_redis-master.yaml ├── Deployment_guestbook_redis-slave.yaml ├── Endpoints_guestbook_frontend.yaml ├── Endpoints_guestbook_redis-master.yaml ├── Endpoints_guestbook_redis-slave.yaml ├── EndpointSlice_guestbook_frontend-bkqbs.yaml ├── EndpointSlice_guestbook_redis-master-hxr5k.yaml ├── EndpointSlice_guestbook_redis-slave-8wt7z.yaml ├── Pod_guestbook_frontend-5fd859dcf6-5nvbm.yaml ├── Pod_guestbook_frontend-5fd859dcf6-j8w94.yaml ├── Pod_guestbook_frontend-5fd859dcf6-s9x8p.yaml ├── Pod_guestbook_redis-master-55d9747c6c-6f9bz.yaml ├── Pod_guestbook_redis-slave-5c6b4c5b47-jnrsr.yaml ├── Pod_guestbook_redis-slave-5c6b4c5b47-xz776.yaml ├── ReplicaSet_guestbook_frontend-5fd859dcf6.yaml ├── ReplicaSet_guestbook_redis-master-55d9747c6c.yaml ├── ReplicaSet_guestbook_redis-slave-5c6b4c5b47.yaml ├── Secret_guestbook_default-token-5vsrb.yaml ├── ServiceAccount_guestbook_default.yaml ├── Service_guestbook_frontend.yaml ├── Service_guestbook_redis-master.yaml └── Service_guestbook_redis-slave.yaml 4 directories, 24 files Crane Export is using a discovery client to see all of the API resources in the specified namespace of the designated cluster and outputing them to the disk in YAML form. This allows workloads to migrate in a non-destructive way.\nGoing forward these manifests will be working on the disk without impacting the active resources in the “source” cluster.\n3. Generate Transformations Crane’s transform command generates tranformations in the form of JSON patches and stores them on the disk in the transform directory (unless overridden using --transform-dir).\ncrane transform Check the transform directory to verify the command worked correctly:\n$ tree -a transform The directory should look similar to the example below:\ntransform └── resources └── guestbook ├── transform-ConfigMap_guestbook_kube-root-ca.crt.yaml ├── transform-Deployment_guestbook_frontend.yaml ├── transform-Deployment_guestbook_redis-master.yaml ├── transform-Deployment_guestbook_redis-slave.yaml ├── transform-Secret_guestbook_default-token-5vsrb.yaml ├── transform-ServiceAccount_guestbook_default.yaml ├── transform-Service_guestbook_frontend.yaml ├── transform-Service_guestbook_redis-master.yaml ├── transform-Service_guestbook_redis-slave.yaml ├── .wh.Endpoints_guestbook_frontend.yaml ├── .wh.Endpoints_guestbook_redis-master.yaml ├── .wh.Endpoints_guestbook_redis-slave.yaml ├── .wh.EndpointSlice_guestbook_frontend-bkqbs.yaml ├── .wh.EndpointSlice_guestbook_redis-master-hxr5k.yaml ├── .wh.EndpointSlice_guestbook_redis-slave-8wt7z.yaml ├── .wh.Pod_guestbook_frontend-5fd859dcf6-5nvbm.yaml ├── .wh.Pod_guestbook_frontend-5fd859dcf6-j8w94.yaml ├── .wh.Pod_guestbook_frontend-5fd859dcf6-s9x8p.yaml ├── .wh.Pod_guestbook_redis-master-55d9747c6c-6f9bz.yaml ├── .wh.Pod_guestbook_redis-slave-5c6b4c5b47-jnrsr.yaml ├── .wh.Pod_guestbook_redis-slave-5c6b4c5b47-xz776.yaml ├── .wh.ReplicaSet_guestbook_frontend-5fd859dcf6.yaml ├── .wh.ReplicaSet_guestbook_redis-master-55d9747c6c.yaml └── .wh.ReplicaSet_guestbook_redis-slave-5c6b4c5b47.yaml 2 directories, 24 files Crane Transform is iterating through the configured plugins and running them against the exported resources from the previous step. View which plugins are configured with Crane Transform list-plugins and optional arguments to those plugins with crane transform optionals.\nExplore what plugins can be configured with the Crane plugin-manager list, install one, and customize the exported resources:\ncrane transform --optional-flags=\u0026#34;add-annotations=custom-crane-annotation=foo\u0026#34; If the flags get difficult to manage via the command-line, specify a --flags-file similar to the example below::\ndebug: false export-dir: myExport transform-dir: myTransform output-dir: myOutput optional-flags: add-annotations: custom-crane-annotation: \u0026#34;foo\u0026#34; 4. Apply Transformations The Crane Apply command takes the exported resources and transformations and renders the results as YAML files that can be applied to another cluster.\ncrane apply Look at one of the transformations created in the last step to better understand the Apply command.\n$ cat transform/resources/guestbook/transform-Deployment_guestbook_frontend.yaml [{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/uid\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/resourceVersion\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/creationTimestamp\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/metadata/generation\u0026#34;},{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/status\u0026#34;}]% When crane applies the transformations for the Guestbook frontend it executes a handful of JSON patches that:\nRemove the UID Remove the resourceVersion Remove the creationTimestamp Remove the generation field Remove the status The leftover data from the source cluster is removed from the final manifests to make them applicable to the destination cluster.\nThe resources are effectively cluster agnostic and ready to be kubectl applied to the chosen cluster or placed under version control to be later managed by GitOps and CI/CD pipelines.\nNote: Additional patches to add/remove/replace additional fields on the resources previously exported are available if optional flags are specified..\nApply the manifests to the destination cluster Apply the manifests prepared for the destination cluster using kubectl directly:\nkubectl --context dest create namespace guestbook kubectl --context dest --namespace guestbook --recursive=true apply -f ./output Note: To change the namespace, use Kustomize.\ncd output/resources/guestbook kustomize init --namespace custom-guestbook --autodetect The result is a kustomization.yaml like the example below:\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ConfigMap_guestbook_kube-root-ca.crt.yaml - Deployment_guestbook_frontend.yaml - Deployment_guestbook_redis-master.yaml - Deployment_guestbook_redis-slave.yaml - Secret_guestbook_default-token-5vsrb.yaml - ServiceAccount_guestbook_default.yaml - Service_guestbook_frontend.yaml - Service_guestbook_redis-master.yaml - Service_guestbook_redis-slave.yaml namespace: custom-guestbook After creating the custom-guestbook namespace, apply the kustomization.yaml with kubectl apply -k.\nNext Steps Read more about Crane. Check out Crane Runner to perform application migrations inside Kubernetes. Cleanup kubectl --context dest delete namespace guestbook Source\n"},{"uri":"http://konveyor.github.io/crane/installation/","title":"Installing Crane","tags":[],"description":"","content":"Follow the procedure below to install the Crane tool.\nStep 1. Install the Crane binary. Enter the following command to install the latest version of Crane binary. curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;\u0026lt;arch\u0026gt;-\u0026lt;os\u0026gt;\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- chmod +x \u0026lt;binary\u0026gt; cp \u0026lt;binary\u0026gt; /usr/bin/crane Crane currently supports three architectures: amd64-linux amd64-darwin arm64-darwin Run the following command to download the latest version of Crane for amd64-linux. curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;amd64-linux\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Run the following command to download the latest version of Crane for amd64-darwin. curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;amd64-darwin\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Run the following command to download the latest version of Crane for arm64-darwin. curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \u0026#34;.assets[] | select(.name | contains(\\\u0026#34;arm64-darwin\\\u0026#34;)) | .browser_download_url\u0026#34; | wget -i- Step 2. Install the most recent version of Crane from the upstream main branch. GOPATHshould be configured to build the project. cd $GOPATH git clone https://github.com/konveyor/crane.git cd crane go build -o crane main.go cp crane /usr/bin/crane Note: Install the released version instead of building from upstream main.\nSource\n"},{"uri":"http://konveyor.github.io/crane/tutorials/migratek8cluster/","title":"Migrating a Kubernetes cluster","tags":[],"description":"","content":"This tutorial is an example of how to use the Konveyor tool Crane to migrate an application (inventory) from the source Kubernetes cluster (src) to the destination cluster (dest).\nRefer to the Crane Documentation for more detailed information.\nNote: In addition to migrating with Crane, it is helpful to push the application to git so it can be automatically deployed to any cluster in the future. This demo includes those steps.\nView details of the destination and source clusters. \u0026gt; $ minikube profile list View the applications on the source cluster. \u0026gt; $ kubectl --context src get namespace View the inventory and postgres services. \u0026gt; $ kubectl --context src --namespace inventory get all View the inventory storage capacity. \u0026gt; $ kubectl --context src --namespace inventory get pvc View the front-end database content. \u0026gt; $ curl http://… Set the context to the source. \u0026gt; $ kubectx src List the application namespaces on the source. \u0026gt; $ kubectl get ns Tip: Use the kubectx and kubectl get ns commands to change the context to the destination and verify the migrating application does not exist.\nCreate an export folder. \u0026gt; $crane export --namespace=inventory View what was extracted from the source cluster into yaml files. \u0026gt; $ tree export Note: These files cannot be imported into another cluster because of the existing IP addresses, timestamps, etc. which should not be pushed to git. View this data using the cat command.\nWhen the application is migrated, this information must be updated to the new cluster.\nClean the manifest files before pushing to git. \u0026gt; $ crane transform list-plugins Note: This example uses the Kubernetes plugin. Use the crane plugin-manager list command to view available plugins to install and translate the manifest into OpenShift for example.\nView available options that can be used to change the manifests before pushing to git. \u0026gt; $ crane transform optionals Create a transform folder using the default Kubernetes transform options. \u0026gt; $ crane transform View the patches in the transform folder that will be applied to the original manifests to create new ones. \u0026gt; $ tree transform Open the patches and verify the data that will be changed before the transform. \u0026gt; $ cat [directory path] Clean up the manifest files using the patches and create an output folder. \u0026gt; $ crane apply View the cleaned manifest files. \u0026gt; $ tree output Copy the cleaned manifests to the git folder for future migrations. \u0026gt; $ crane apply -o Change the context to the destination. \u0026gt; $ kubectx dest Create a new namespace. \u0026gt; $ kubectl --context dest create ns inventory \\ namespace/inventory created Verify the name of the pvc needed for migration. \u0026gt; $ kubectl --context src --namespace inventory get pvc Begin the migration. \u0026gt; $ crane transfer-pvc --source-context=src \\ --pvc-name=postgres-pv-claim --destination-context=dest Tip: Perform the migration while the application is running and then again during a maintenance window when the application has been shut down to only migrate over the delta between the migrations. 22. List the contents of the demo directory.\n\u0026gt; $ ls View the migrated files. \u0026gt; $ tree output View the argo inventory file and verify the git repoURL, namespace, and server. \u0026gt; $ cat inventory.argo.yaml Copy the file into the argo cd. \u0026gt; $ kubectl --context dest --namespace argocd apply -f inventory.argo.yaml Open the Argo CD user interface. Open the migrated application. Verify Argo picked up and provisioned all the manifests from the git repo. Click the Sync button at the top of the screen where the application can be resynced from the git repo. Source\n"},{"uri":"http://konveyor.github.io/","title":"Konveyor Documentation","tags":[],"description":"","content":"Why Konveyor? Sysadmins and Developers are tired of the words “Digital Transformation.” They don’t want to hear about the “five keys to digital transformation” or the “seven must-haves to transform.” They are also tired of every vendor presenting their framework and methodology for digital transformation bundled with a bunch of proprietary tools.\nDevelopers and sysadmins want to learn how to actually transform their applications and infrastructure so they can take advantage of new technologies to deliver new capabilities faster, at greater scale, and with higher quality while reducing technical debt. It is clear that one of the technologies that underpins the future of applications and infrastructure is Kubernetes - an open-source system for automating deployment, scaling, and management of containerized applications.\nThe Konveyor community exists to accelerate sysadmins and developers\u0026rsquo; journey to Kubernetes. Konveyor is a community of people passionate about helping others modernize and migrate their applications for Kubernetes by building tools, identifying patterns, and providing advice.\nKonveyor projects Konveyor currently consists of five tools or projects:\nForklift Crane Move2Kube Tackle Pelorus Forklift is focused on migrating virtual machines to Kubernetes and provides the ability to migrate multiple virtual machines to KubeVirt with minimal downtime. It allows organizations to rehost, or “lift and shift\u0026rsquo;\u0026rsquo; applications residing on these VMs. While rehosting doesn’t provide the same depth of benefits as replatforming or refactoring, it’s the first step in the right direction. It’s often useful to have all the unmodified development workloads in Kubernetes as a basecamp for further transformations, or in cases where development teams may not have the ability to change or modify code, such as with vendor provided software. Rehosting also helps teams enjoy the benefits of the new platform with less friction in improving process and culture.\nThe downstream version of this tool, Migration Toolkit for Virtualization (MTV) is available to Red Hat customers interested in moving vSphere virtual machines to OpenShift Virtualization. As of February 2021, it is available as tech preview.\nCrane is another rehosting tool that meets a different use case. It allows organizations to migrate applications between Kubernetes clusters. There are many times when developers and operations teams want to migrate between older and newer versions of Kubernetes, evacuate a cluster, or migrate to different underlying infrastructure. In an ideal scenario, this would be a redeployment of the application, but in reality we have found that many users need a solution for migrating persistent data and the objects within Kubernetes namespaces continuously.\nThe downstream version of Crane, Migration Toolkit for Containers (MTC), is available for Red Hat OpenShift customers interested in moving from version 3.x to 4, as well as from different clusters of version 4. It is fully supported and available on OpenShift Operator Hub.\nMove2Kube is a project that allows customers to replatform their applications to Kubernetes orchestrated platforms. Replatforming, or “lift, tinker, and shift”, involves changing an underlying technology used by an application while minimizing the need for code change. One area where replatforming is taking place is in the consolidation of container orchestration platforms to Kubernetes. Due to this consolidation, the Move2Kube project was started to focus on accelerating the process of replatform to Kubernetes from platforms such as Swarm and Cloud Foundry. The project translates existing artifacts to Kubernetes artifacts to speed up the process of being able to run applications on Kubernetes.\nTackle provides a series of interrelated tools that allows users to assess, analyze, and ultimately move their applications onto a Kubernetes orchestrated platform. Often considered the most challenging application modernization strategy, adapting applications to a containerized runtime, also offers the largest potential long term impact. This strategy involves making changes to the application and development to take advantage of cloud native capabilities. The tool helps assess the depth of the changes ranging from minimal fixes to adapt the application to containers to a full rewrite of the application in more modern container-friendly runtimes.The Tackle project provides tools that inventory an application environment and identify which workloads are most suitable for refactoring into containers. A common application inventory can also be generated which is then made available as a basis for code execution. The team that is catalyzing this project has experience in these areas from working on tools such as Pathfinder and Windup and will be bringing these experiences to their work on the Tackle project.\nThe downstream version of Tackle, Migration Toolkit for Applications (MTA), is an assembly of tools that support large-scale Java application modernization and migration projects across a broad range of transformations and use cases. MTA accelerates application code analysis, supports effort estimation, accelerates code migration, and helps users move applications to a variety of platforms including OpenShift.\nPelorus is focused around measuring the improvement that moving and modernizing actually delivers as described in the Accelerate book, the reference in DevOps. The community feels strongly that being able to measure the impact of rehosting, replatform, refactoring, and changing processes and culture is vital to proving value. Pelorus is a project focused on measuring the key metrics to software delivery performance (lead time for change, deployment frequency, mean time to restore, and change failure rate) and enabling metrics driven transformation.\nTogether, these five projects comprise today the Konveyor community. Moving forward, it is inevitable that demand for each will align with evolving migration strategies and technology trends. For example, whereas rehosting with Forklift defined use cases may reach heightened demand in the short term, this project could also very well lose traction as Virtual Machines are fully containerized across the landscape in the longer term. New tools could then hypothetically emerge in the Konveyor community empowering users to further extend these containerized applications to edge computing environments, perhaps relying on AI/ML and event-driven architectures. That being said, it is widely expected that there will be ebb and flow with existing project usage, as well as the introduction of entirely new projects altogether.\n-Content taken from the Konveyor Messaging Guide\nSource\n"},{"uri":"http://konveyor.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"http://konveyor.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]